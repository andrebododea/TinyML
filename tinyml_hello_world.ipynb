{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "colab-github-demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrebododea/TinyML/blob/master/tinyml_hello_world.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-pVhOfzLx9us"
      },
      "source": [
        "# Create And Convert A TensorFlow Model\n",
        "This notebook lays out just a basic \"hello world\" machine learning project for an embedded environment, using Keras and TensorFlow Lite.\n",
        "\n",
        "\n",
        "The goal here is  to train a model to take a value, x, and predict its sine, y. This is obviously just a toy example, but it will then be run on an embedded device. The end result will be that the sine wave will run smoothly from -1 to 1 and back, making it perfect for oscillating an LED on the microcontroller. The target platform will be an Arduino Nano 33 BLE Sense, running an ARM Cortex-M4 processor\n",
        "\n",
        "This project is based on the following [notebook](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb) from the TensorFlow examples repo. Everything has been hand-coded and reproduced manually for learning purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKJ4bd5rt1wy"
      },
      "source": [
        "# Import dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtQz4cY-NGwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorFlow is an open source ML library\n",
        "import tensorflow as tf\n",
        "# NumPy is a math library\n",
        "import numpy as np\n",
        "# Matplotlib is a graphing library\n",
        "import matplotlib.pyplot as plt\n",
        "# math is Python's math library\n",
        "import math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3VQqVi-3ScBC",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PUZ73euOXlS",
        "colab_type": "text"
      },
      "source": [
        "# Generate data\n",
        "\n",
        "The following code will generate a set of random `x` values, calculate their sine values, and display them on a graph:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEPjUSJZOxOW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "4b3a3681-5399-4c01-99c8-f2a1749e7d6a"
      },
      "source": [
        "# Define the number of sample datapoints\n",
        "SAMPLES = 1000\n",
        "\n",
        "# Set a seed value for the random number generator\n",
        "np.random.seed(1337)\n",
        "\n",
        "# Generate a uniformly distributed set of random numbers in the range from  \n",
        "# 0 to 2Ï€, which covers a complete sine wave oscillation\n",
        "x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)\n",
        "\n",
        "# Shuffle the values to guarantee they're not in order\n",
        "np.random.shuffle(x_values)\n",
        "\n",
        "# Calculate the corresponding sine values\n",
        "y_values = np.sin(x_values)\n",
        "\n",
        "# Plot our data. The 'b.' argument tells the library to print blue dots\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cT6iJPnQVvoj",
        "colab_type": "text"
      },
      "source": [
        "# Adding noise\n",
        "The data generated is perfect....almost too perfect. We're going to want to add some noise to it in order to simulate real-world, messy data. \n",
        "\n",
        "So now add some random noise to each value, then draw a new graph."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh_pa0J2VcEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "796c7d34-37be-4dbd-a580-3a859490085f"
      },
      "source": [
        "# Add a small random number to each y value\n",
        "y_values += 0.1 * np.random.randn(*y_values.shape)\n",
        "\n",
        "# Now plot the data\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKIpSJ/w/QTgoZsK+iUsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suq5OiCKubmg",
        "colab_type": "text"
      },
      "source": [
        "# Split our data\n",
        "We now have a noisy dataset that approximates \"real world data\". We'll use this to train our model.\n",
        "\n",
        "To evaluate the accuracy of the model we train, we'll need to compare its predictions to real data and check how well they match up. This evaluation happens during training (where it is referred to as validation), and then after training (referred to as testing). \n",
        "\n",
        "So we'll need to set aside some data to use for evaluation and testing - 20% and 20% respectively. The remaining 60% is used to train the model. This is a typical split for training models. \n",
        "\n",
        "The following code splits the data then plots each set as a different color. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtsMs4t4W6Bo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "94239c32-19fd-4bcc-dd5f-099e822bbc40"
      },
      "source": [
        "# Calculate the indeces for the training section of the data, and the test section.\n",
        "# Validation will be from 0 to TRAIN_SPLIT, then TRAIN_SPLIT to TEST_SPLIT will be \n",
        "# the training data. Then the rest is testing data.\n",
        "TRAIN_SPLIT = int(0.6 * SAMPLES)\n",
        "TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)\n",
        "\n",
        "# Use np.split to chop our data into three parts\n",
        "# The second argument to np.split is an array of indices where the data will be \n",
        "# split. We provide two indices, so the data will be divided into three chunks.\n",
        "x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "# Double check that splits add up correctly\n",
        "assert (x_train.size + x_validate.size + x_test.size) == SAMPLES\n",
        "\n",
        "# Plot the data in each partition in different colors\n",
        "plt.plot(x_train, y_train, 'b.', label=\"Train\")\n",
        "plt.plot(x_test, y_test, 'r.', label=\"Test\")\n",
        "plt.plot(x_validate, y_validate, 'y.', label=\"Validate\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZhkXKWsxeZN",
        "colab_type": "text"
      },
      "source": [
        "# Design a model\n",
        "We're going to build a model that takes an input value (in this case `x`) and use it to predict a numeric output value (the sine of `x`). This  type of problem is called a _regression_.\n",
        "\n",
        "To achieve this, we'll create a simple neural network. It will use _layers of neurons_ to attempt to learn any patterns underlying the training data, so it can make predictions.\n",
        "\n",
        "To begin with, we'll define two layers. The first layer takes a single input (our `x` value) and runs it through 16 neurones. Based on this input, each neuron will become _activated_ to a certain degree based on its internal state (its _weight_ and _bias_ values). A neuron's degree of activation is expressed as a number.\n",
        "\n",
        "The activation numbers from our first layer will be fed as inputs to our second layer, which is a single neuron. It will apply its own weights and bias to these inputs and calculate its own activation, which will be output as our `y` value. \n",
        "\n",
        "The code in the following cell defines our model using Keras, TensorFlow's high-level API for creating deep learning networks. Once the network is defined, we _compile_ it, specifying parameters that determine how it will be trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGEGupINxZak",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We'll use Keras to create a simple model architecture\n",
        "from tensorflow.keras import layers\n",
        "model_1 = tf.keras.Sequential()\n",
        "\n",
        "# First layer takes a scalar input and feeds it through 16 \"neurons\". The\n",
        "# neurons decide whether to activate based on the 'relu' activation function.\n",
        "model_1.add(layers.Dense(16, activation='relu', input_shape=(1,)))\n",
        "\n",
        "# Final layer is a single neuron, since we want to output a single value\n",
        "model_1.add(layers.Dense(1))\n",
        "\n",
        "# Compile the model using a standard optimizer and loss function for regression\n",
        "model_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-dmcfMBzyqs",
        "colab_type": "text"
      },
      "source": [
        "So what's going on here?\n",
        "\n",
        "First, we're creating a Sequential model using Keras. This just means a model in which each layer of neurons is stacked on top of the next. Then we define two layers.\n",
        "\n",
        "## First Layer\n",
        "\n",
        "The first layer has a single input - our `x` value - and 16 neurons. It's a Dense layer, also known as a fully connected layer, meaning the input will be fed into every single one of its neurons during inference, when we're making predictions. \n",
        "\n",
        "The _activation function_ is a mathematical function used to shape the output of the neuron. In our network, we're using an activation function called _rectified lineaer unit_ or _ReLU_ for short. ReLU returns whichever is the larger value: its input, or zero. If its input value is negative, ReLU returns zero. If its input value is above zero, ReLU returns it unchanged. \n",
        "\n",
        "Without an activation function, the neuron's output would always be a linear function of its input. This would mean that the network could model only linear relationships in which the ratio between x and y remains the same across the entire range of values. This would prevent a network from modeling our sine wave, because a sine wave is nonlinear. And since ReLU is nonlinear, it allows multiple layers of neurons to join forces and model complex nonlinear relationships, in which the y value doesn't increase by the same amount for every increment of x.\n",
        "\n",
        "## Second layer\n",
        "The activation numbers from our first layer will be fed as inputs to our second layer. Because this layer is a single neuron, it will receive 16 inputs, one for each of the neurons in the previous layer. Its purpose is to combine all of the activations from the previous layer into a single output value. Since this is our output layer, we don't specify an activation function - we just want the raw result.\n",
        "\n",
        "Because this neuron has multiple inputs, it has a corresponding weight value for each. The neuron's output is calculated by the following formula, shown in Python:\n",
        "` output = sum((inputs * weights)) * bias`. The output value is obtained by multiplying each input with its corresponding weight, summing the results, and then adding in the neuron's bias. **The network's weights and biases are learned during training.** \n",
        "\n",
        "## Compiling\n",
        "The `compile()` step in the code configures some important arguments used in the training process, and prepares the model to be trained.\n",
        "\n",
        "The `optimizer` argument specifies the algorithm that will adjust the network to model its input during training. There are several choices, and finding the best one often comes down to experimentation. \n",
        "\n",
        "The `loss` argument specifies the method used during training to calculate how far the network's predictions are from reality. This method is called a _loss function_. Here, we're using `mse` or _mean squared error_. This loss function is used in the case of regressino problems, for which we're trying to predict a number.\n",
        "\n",
        "The `metrics` argument allows us to specify some additional functions that are used to judge the performance of our model. We specify `mae` or _mean absolute error_, which is helpful for measuring the performance of a regression model. This metric will be measured during training, and we'll have access to the results after training is done. \n",
        "\n",
        "After compiling the model, we can use the following line to print some summary information about its architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiHxIOyNzydh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "330970f5-6303-454b-90de-861899bd8cf3"
      },
      "source": [
        "# Print a summary of the model's architecture\n",
        "model_1.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 16)                32        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 17        \n",
            "=================================================================\n",
            "Total params: 49\n",
            "Trainable params: 49\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzIlpRg-4Qmj",
        "colab_type": "text"
      },
      "source": [
        "The table shows the layers of the network, their output shapes, and their numbers of parameters. **The size of a network - how much memory it takes up - depends mostly on its number of parameters, meaning its total number of weights and biases. This can be a useful metric when discussing model size and complexity.**\n",
        "\n",
        "\n",
        "# Training our model\n",
        "After defining our model, it's time to train it and then evaluate its performance to see how well it works. The metrics will tell us if it's good enough, or if we need to make changes to our design and train it again.\n",
        "\n",
        "To train a model in Keras, we just call its `fit()` method, passing all of our data and some other important arguments. The code in the next cell shows how:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xxtc9Z-fy1iM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a9b4671-6d8b-45b6-abdc-3b81ec61fb30"
      },
      "source": [
        "# Train the model on our training data while validating on our validation set \n",
        "history_1 = model_1.fit(x_train, y_train, epochs=1000, batch_size=16,\n",
        "                        validation_data=(x_validate, y_validate))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.4094 - mae: 0.5590 - val_loss: 0.4553 - val_mae: 0.5831\n",
            "Epoch 2/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3715 - mae: 0.5277 - val_loss: 0.4178 - val_mae: 0.5665\n",
            "Epoch 3/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3370 - mae: 0.5077 - val_loss: 0.3845 - val_mae: 0.5410\n",
            "Epoch 4/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.3116 - mae: 0.4877 - val_loss: 0.3544 - val_mae: 0.5108\n",
            "Epoch 5/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2890 - mae: 0.4691 - val_loss: 0.3254 - val_mae: 0.4908\n",
            "Epoch 6/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2680 - mae: 0.4521 - val_loss: 0.2985 - val_mae: 0.4765\n",
            "Epoch 7/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2464 - mae: 0.4354 - val_loss: 0.2767 - val_mae: 0.4602\n",
            "Epoch 8/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2307 - mae: 0.4229 - val_loss: 0.2585 - val_mae: 0.4406\n",
            "Epoch 9/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2174 - mae: 0.4071 - val_loss: 0.2472 - val_mae: 0.4392\n",
            "Epoch 10/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.2060 - mae: 0.3991 - val_loss: 0.2334 - val_mae: 0.4267\n",
            "Epoch 11/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1966 - mae: 0.3899 - val_loss: 0.2192 - val_mae: 0.4115\n",
            "Epoch 12/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1875 - mae: 0.3798 - val_loss: 0.2130 - val_mae: 0.4056\n",
            "Epoch 13/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1841 - mae: 0.3755 - val_loss: 0.2009 - val_mae: 0.3913\n",
            "Epoch 14/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1771 - mae: 0.3673 - val_loss: 0.1948 - val_mae: 0.3802\n",
            "Epoch 15/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1755 - mae: 0.3659 - val_loss: 0.1893 - val_mae: 0.3762\n",
            "Epoch 16/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1747 - mae: 0.3610 - val_loss: 0.1890 - val_mae: 0.3767\n",
            "Epoch 17/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1712 - mae: 0.3563 - val_loss: 0.1823 - val_mae: 0.3668\n",
            "Epoch 18/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1702 - mae: 0.3562 - val_loss: 0.1804 - val_mae: 0.3653\n",
            "Epoch 19/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1676 - mae: 0.3522 - val_loss: 0.1795 - val_mae: 0.3596\n",
            "Epoch 20/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1675 - mae: 0.3494 - val_loss: 0.1760 - val_mae: 0.3578\n",
            "Epoch 21/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1676 - mae: 0.3496 - val_loss: 0.1752 - val_mae: 0.3573\n",
            "Epoch 22/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1655 - mae: 0.3466 - val_loss: 0.1738 - val_mae: 0.3552\n",
            "Epoch 23/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1663 - mae: 0.3463 - val_loss: 0.1733 - val_mae: 0.3542\n",
            "Epoch 24/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1656 - mae: 0.3449 - val_loss: 0.1771 - val_mae: 0.3589\n",
            "Epoch 25/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1676 - mae: 0.3443 - val_loss: 0.1749 - val_mae: 0.3561\n",
            "Epoch 26/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1655 - mae: 0.3441 - val_loss: 0.1708 - val_mae: 0.3501\n",
            "Epoch 27/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1662 - mae: 0.3434 - val_loss: 0.1715 - val_mae: 0.3512\n",
            "Epoch 28/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1654 - mae: 0.3425 - val_loss: 0.1713 - val_mae: 0.3468\n",
            "Epoch 29/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1669 - mae: 0.3414 - val_loss: 0.1714 - val_mae: 0.3508\n",
            "Epoch 30/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1652 - mae: 0.3416 - val_loss: 0.1690 - val_mae: 0.3466\n",
            "Epoch 31/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1665 - mae: 0.3407 - val_loss: 0.1705 - val_mae: 0.3494\n",
            "Epoch 32/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1663 - mae: 0.3415 - val_loss: 0.1693 - val_mae: 0.3476\n",
            "Epoch 33/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1659 - mae: 0.3408 - val_loss: 0.1683 - val_mae: 0.3454\n",
            "Epoch 34/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1666 - mae: 0.3402 - val_loss: 0.1685 - val_mae: 0.3460\n",
            "Epoch 35/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1685 - mae: 0.3410 - val_loss: 0.1700 - val_mae: 0.3484\n",
            "Epoch 36/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1641 - mae: 0.3407 - val_loss: 0.1679 - val_mae: 0.3443\n",
            "Epoch 37/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1666 - mae: 0.3396 - val_loss: 0.1678 - val_mae: 0.3441\n",
            "Epoch 38/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1639 - mae: 0.3399 - val_loss: 0.1676 - val_mae: 0.3441\n",
            "Epoch 39/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1647 - mae: 0.3382 - val_loss: 0.1739 - val_mae: 0.3522\n",
            "Epoch 40/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1645 - mae: 0.3397 - val_loss: 0.1672 - val_mae: 0.3429\n",
            "Epoch 41/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1650 - mae: 0.3385 - val_loss: 0.1707 - val_mae: 0.3485\n",
            "Epoch 42/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1649 - mae: 0.3398 - val_loss: 0.1670 - val_mae: 0.3429\n",
            "Epoch 43/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1637 - mae: 0.3385 - val_loss: 0.1694 - val_mae: 0.3469\n",
            "Epoch 44/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1650 - mae: 0.3391 - val_loss: 0.1670 - val_mae: 0.3431\n",
            "Epoch 45/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1638 - mae: 0.3374 - val_loss: 0.1791 - val_mae: 0.3565\n",
            "Epoch 46/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1663 - mae: 0.3400 - val_loss: 0.1696 - val_mae: 0.3471\n",
            "Epoch 47/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1641 - mae: 0.3376 - val_loss: 0.1686 - val_mae: 0.3460\n",
            "Epoch 48/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1649 - mae: 0.3377 - val_loss: 0.1688 - val_mae: 0.3461\n",
            "Epoch 49/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.3361 - val_loss: 0.1665 - val_mae: 0.3414\n",
            "Epoch 50/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1632 - mae: 0.3367 - val_loss: 0.1665 - val_mae: 0.3422\n",
            "Epoch 51/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1649 - mae: 0.3370 - val_loss: 0.1666 - val_mae: 0.3424\n",
            "Epoch 52/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1618 - mae: 0.3354 - val_loss: 0.1709 - val_mae: 0.3480\n",
            "Epoch 53/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1643 - mae: 0.3367 - val_loss: 0.1661 - val_mae: 0.3407\n",
            "Epoch 54/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1650 - mae: 0.3363 - val_loss: 0.1686 - val_mae: 0.3453\n",
            "Epoch 55/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1656 - mae: 0.3364 - val_loss: 0.1689 - val_mae: 0.3456\n",
            "Epoch 56/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.3371 - val_loss: 0.1686 - val_mae: 0.3454\n",
            "Epoch 57/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1638 - mae: 0.3370 - val_loss: 0.1665 - val_mae: 0.3424\n",
            "Epoch 58/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1631 - mae: 0.3367 - val_loss: 0.1660 - val_mae: 0.3410\n",
            "Epoch 59/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1649 - mae: 0.3362 - val_loss: 0.1657 - val_mae: 0.3404\n",
            "Epoch 60/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1636 - mae: 0.3364 - val_loss: 0.1672 - val_mae: 0.3434\n",
            "Epoch 61/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1651 - mae: 0.3355 - val_loss: 0.1661 - val_mae: 0.3415\n",
            "Epoch 62/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1627 - mae: 0.3350 - val_loss: 0.1666 - val_mae: 0.3426\n",
            "Epoch 63/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1647 - mae: 0.3368 - val_loss: 0.1660 - val_mae: 0.3415\n",
            "Epoch 64/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1634 - mae: 0.3358 - val_loss: 0.1653 - val_mae: 0.3396\n",
            "Epoch 65/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1647 - mae: 0.3348 - val_loss: 0.1664 - val_mae: 0.3421\n",
            "Epoch 66/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1622 - mae: 0.3355 - val_loss: 0.1686 - val_mae: 0.3447\n",
            "Epoch 67/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3345 - val_loss: 0.1666 - val_mae: 0.3423\n",
            "Epoch 68/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3360 - val_loss: 0.1654 - val_mae: 0.3403\n",
            "Epoch 69/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1615 - mae: 0.3348 - val_loss: 0.1653 - val_mae: 0.3385\n",
            "Epoch 70/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1633 - mae: 0.3353 - val_loss: 0.1651 - val_mae: 0.3396\n",
            "Epoch 71/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1633 - mae: 0.3357 - val_loss: 0.1660 - val_mae: 0.3416\n",
            "Epoch 72/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1636 - mae: 0.3358 - val_loss: 0.1649 - val_mae: 0.3389\n",
            "Epoch 73/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1634 - mae: 0.3341 - val_loss: 0.1709 - val_mae: 0.3468\n",
            "Epoch 74/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1645 - mae: 0.3362 - val_loss: 0.1656 - val_mae: 0.3407\n",
            "Epoch 75/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3345 - val_loss: 0.1668 - val_mae: 0.3383\n",
            "Epoch 76/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1609 - mae: 0.3316 - val_loss: 0.1709 - val_mae: 0.3466\n",
            "Epoch 77/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1618 - mae: 0.3350 - val_loss: 0.1648 - val_mae: 0.3391\n",
            "Epoch 78/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1645 - mae: 0.3353 - val_loss: 0.1651 - val_mae: 0.3399\n",
            "Epoch 79/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1628 - mae: 0.3347 - val_loss: 0.1651 - val_mae: 0.3380\n",
            "Epoch 80/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1627 - mae: 0.3337 - val_loss: 0.1687 - val_mae: 0.3444\n",
            "Epoch 81/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1629 - mae: 0.3357 - val_loss: 0.1647 - val_mae: 0.3387\n",
            "Epoch 82/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1625 - mae: 0.3340 - val_loss: 0.1644 - val_mae: 0.3377\n",
            "Epoch 83/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1642 - mae: 0.3355 - val_loss: 0.1647 - val_mae: 0.3391\n",
            "Epoch 84/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1603 - mae: 0.3312 - val_loss: 0.1708 - val_mae: 0.3463\n",
            "Epoch 85/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1629 - mae: 0.3340 - val_loss: 0.1646 - val_mae: 0.3388\n",
            "Epoch 86/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1639 - mae: 0.3343 - val_loss: 0.1647 - val_mae: 0.3391\n",
            "Epoch 87/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1642 - mae: 0.3339 - val_loss: 0.1644 - val_mae: 0.3384\n",
            "Epoch 88/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1635 - mae: 0.3343 - val_loss: 0.1642 - val_mae: 0.3377\n",
            "Epoch 89/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3330 - val_loss: 0.1642 - val_mae: 0.3378\n",
            "Epoch 90/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1634 - mae: 0.3344 - val_loss: 0.1653 - val_mae: 0.3403\n",
            "Epoch 91/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1620 - mae: 0.3330 - val_loss: 0.1641 - val_mae: 0.3377\n",
            "Epoch 92/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1598 - mae: 0.3329 - val_loss: 0.1654 - val_mae: 0.3404\n",
            "Epoch 93/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1618 - mae: 0.3339 - val_loss: 0.1645 - val_mae: 0.3370\n",
            "Epoch 94/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1622 - mae: 0.3336 - val_loss: 0.1641 - val_mae: 0.3379\n",
            "Epoch 95/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3324 - val_loss: 0.1650 - val_mae: 0.3396\n",
            "Epoch 96/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3330 - val_loss: 0.1647 - val_mae: 0.3392\n",
            "Epoch 97/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3331 - val_loss: 0.1640 - val_mae: 0.3378\n",
            "Epoch 98/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1628 - mae: 0.3327 - val_loss: 0.1636 - val_mae: 0.3367\n",
            "Epoch 99/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1617 - mae: 0.3321 - val_loss: 0.1636 - val_mae: 0.3363\n",
            "Epoch 100/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1611 - mae: 0.3325 - val_loss: 0.1664 - val_mae: 0.3412\n",
            "Epoch 101/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1601 - mae: 0.3313 - val_loss: 0.1636 - val_mae: 0.3366\n",
            "Epoch 102/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1606 - mae: 0.3303 - val_loss: 0.1682 - val_mae: 0.3429\n",
            "Epoch 103/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1596 - mae: 0.3305 - val_loss: 0.1642 - val_mae: 0.3383\n",
            "Epoch 104/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1622 - mae: 0.3323 - val_loss: 0.1633 - val_mae: 0.3357\n",
            "Epoch 105/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1606 - mae: 0.3303 - val_loss: 0.1678 - val_mae: 0.3422\n",
            "Epoch 106/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1602 - mae: 0.3303 - val_loss: 0.1701 - val_mae: 0.3446\n",
            "Epoch 107/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3321 - val_loss: 0.1651 - val_mae: 0.3396\n",
            "Epoch 108/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1643 - mae: 0.3324 - val_loss: 0.1671 - val_mae: 0.3418\n",
            "Epoch 109/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1613 - mae: 0.3323 - val_loss: 0.1664 - val_mae: 0.3410\n",
            "Epoch 110/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3323 - val_loss: 0.1635 - val_mae: 0.3356\n",
            "Epoch 111/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3311 - val_loss: 0.1672 - val_mae: 0.3417\n",
            "Epoch 112/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1621 - mae: 0.3325 - val_loss: 0.1645 - val_mae: 0.3388\n",
            "Epoch 113/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1619 - mae: 0.3328 - val_loss: 0.1633 - val_mae: 0.3363\n",
            "Epoch 114/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3330 - val_loss: 0.1635 - val_mae: 0.3368\n",
            "Epoch 115/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1615 - mae: 0.3309 - val_loss: 0.1634 - val_mae: 0.3365\n",
            "Epoch 116/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1622 - mae: 0.3310 - val_loss: 0.1661 - val_mae: 0.3403\n",
            "Epoch 117/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1628 - mae: 0.3322 - val_loss: 0.1631 - val_mae: 0.3360\n",
            "Epoch 118/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1607 - mae: 0.3305 - val_loss: 0.1637 - val_mae: 0.3346\n",
            "Epoch 119/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1628 - mae: 0.3308 - val_loss: 0.1629 - val_mae: 0.3355\n",
            "Epoch 120/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3313 - val_loss: 0.1659 - val_mae: 0.3396\n",
            "Epoch 121/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1602 - mae: 0.3296 - val_loss: 0.1683 - val_mae: 0.3422\n",
            "Epoch 122/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3311 - val_loss: 0.1629 - val_mae: 0.3355\n",
            "Epoch 123/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1618 - mae: 0.3310 - val_loss: 0.1628 - val_mae: 0.3352\n",
            "Epoch 124/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3307 - val_loss: 0.1626 - val_mae: 0.3342\n",
            "Epoch 125/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - mae: 0.3314 - val_loss: 0.1644 - val_mae: 0.3380\n",
            "Epoch 126/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1611 - mae: 0.3319 - val_loss: 0.1626 - val_mae: 0.3349\n",
            "Epoch 127/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1602 - mae: 0.3304 - val_loss: 0.1631 - val_mae: 0.3360\n",
            "Epoch 128/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.3300 - val_loss: 0.1629 - val_mae: 0.3356\n",
            "Epoch 129/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1608 - mae: 0.3301 - val_loss: 0.1651 - val_mae: 0.3385\n",
            "Epoch 130/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1613 - mae: 0.3306 - val_loss: 0.1626 - val_mae: 0.3349\n",
            "Epoch 131/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1629 - mae: 0.3307 - val_loss: 0.1646 - val_mae: 0.3382\n",
            "Epoch 132/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1610 - mae: 0.3305 - val_loss: 0.1624 - val_mae: 0.3338\n",
            "Epoch 133/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1623 - mae: 0.3300 - val_loss: 0.1652 - val_mae: 0.3388\n",
            "Epoch 134/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1612 - mae: 0.3299 - val_loss: 0.1638 - val_mae: 0.3372\n",
            "Epoch 135/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1599 - mae: 0.3300 - val_loss: 0.1627 - val_mae: 0.3337\n",
            "Epoch 136/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1590 - mae: 0.3287 - val_loss: 0.1672 - val_mae: 0.3406\n",
            "Epoch 137/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1593 - mae: 0.3289 - val_loss: 0.1623 - val_mae: 0.3337\n",
            "Epoch 138/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3301 - val_loss: 0.1623 - val_mae: 0.3340\n",
            "Epoch 139/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1600 - mae: 0.3296 - val_loss: 0.1643 - val_mae: 0.3376\n",
            "Epoch 140/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1603 - mae: 0.3306 - val_loss: 0.1620 - val_mae: 0.3337\n",
            "Epoch 141/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1593 - mae: 0.3271 - val_loss: 0.1723 - val_mae: 0.3449\n",
            "Epoch 142/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1607 - mae: 0.3293 - val_loss: 0.1620 - val_mae: 0.3336\n",
            "Epoch 143/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1610 - mae: 0.3279 - val_loss: 0.1676 - val_mae: 0.3406\n",
            "Epoch 144/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1627 - mae: 0.3290 - val_loss: 0.1655 - val_mae: 0.3383\n",
            "Epoch 145/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1607 - mae: 0.3298 - val_loss: 0.1630 - val_mae: 0.3355\n",
            "Epoch 146/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - mae: 0.3282 - val_loss: 0.1618 - val_mae: 0.3324\n",
            "Epoch 147/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1598 - mae: 0.3284 - val_loss: 0.1649 - val_mae: 0.3376\n",
            "Epoch 148/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1600 - mae: 0.3301 - val_loss: 0.1617 - val_mae: 0.3324\n",
            "Epoch 149/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1596 - mae: 0.3279 - val_loss: 0.1668 - val_mae: 0.3395\n",
            "Epoch 150/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - mae: 0.3303 - val_loss: 0.1615 - val_mae: 0.3323\n",
            "Epoch 151/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1609 - mae: 0.3280 - val_loss: 0.1636 - val_mae: 0.3363\n",
            "Epoch 152/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1615 - mae: 0.3288 - val_loss: 0.1632 - val_mae: 0.3358\n",
            "Epoch 153/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1600 - mae: 0.3289 - val_loss: 0.1617 - val_mae: 0.3323\n",
            "Epoch 154/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3264 - val_loss: 0.1617 - val_mae: 0.3331\n",
            "Epoch 155/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1593 - mae: 0.3269 - val_loss: 0.1630 - val_mae: 0.3354\n",
            "Epoch 156/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1607 - mae: 0.3276 - val_loss: 0.1616 - val_mae: 0.3329\n",
            "Epoch 157/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1602 - mae: 0.3278 - val_loss: 0.1611 - val_mae: 0.3315\n",
            "Epoch 158/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1596 - mae: 0.3269 - val_loss: 0.1635 - val_mae: 0.3355\n",
            "Epoch 159/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1600 - mae: 0.3282 - val_loss: 0.1654 - val_mae: 0.3374\n",
            "Epoch 160/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - mae: 0.3270 - val_loss: 0.1671 - val_mae: 0.3392\n",
            "Epoch 161/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1594 - mae: 0.3255 - val_loss: 0.1633 - val_mae: 0.3353\n",
            "Epoch 162/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1605 - mae: 0.3280 - val_loss: 0.1615 - val_mae: 0.3328\n",
            "Epoch 163/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1604 - mae: 0.3287 - val_loss: 0.1626 - val_mae: 0.3346\n",
            "Epoch 164/1000\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.1603 - mae: 0.3278 - val_loss: 0.1614 - val_mae: 0.3325\n",
            "Epoch 165/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.3266 - val_loss: 0.1653 - val_mae: 0.3370\n",
            "Epoch 166/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3275 - val_loss: 0.1608 - val_mae: 0.3308\n",
            "Epoch 167/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3270 - val_loss: 0.1609 - val_mae: 0.3308\n",
            "Epoch 168/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1627 - mae: 0.3271 - val_loss: 0.1666 - val_mae: 0.3384\n",
            "Epoch 169/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1595 - mae: 0.3277 - val_loss: 0.1622 - val_mae: 0.3339\n",
            "Epoch 170/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1591 - mae: 0.3272 - val_loss: 0.1608 - val_mae: 0.3312\n",
            "Epoch 171/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1609 - mae: 0.3260 - val_loss: 0.1619 - val_mae: 0.3335\n",
            "Epoch 172/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3266 - val_loss: 0.1673 - val_mae: 0.3393\n",
            "Epoch 173/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1592 - mae: 0.3285 - val_loss: 0.1614 - val_mae: 0.3325\n",
            "Epoch 174/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1599 - mae: 0.3266 - val_loss: 0.1610 - val_mae: 0.3318\n",
            "Epoch 175/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.3251 - val_loss: 0.1696 - val_mae: 0.3412\n",
            "Epoch 176/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1610 - mae: 0.3275 - val_loss: 0.1609 - val_mae: 0.3312\n",
            "Epoch 177/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - mae: 0.3275 - val_loss: 0.1634 - val_mae: 0.3354\n",
            "Epoch 178/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1596 - mae: 0.3274 - val_loss: 0.1609 - val_mae: 0.3311\n",
            "Epoch 179/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3274 - val_loss: 0.1607 - val_mae: 0.3307\n",
            "Epoch 180/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1581 - mae: 0.3254 - val_loss: 0.1610 - val_mae: 0.3303\n",
            "Epoch 181/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1608 - mae: 0.3267 - val_loss: 0.1608 - val_mae: 0.3312\n",
            "Epoch 182/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1577 - mae: 0.3259 - val_loss: 0.1613 - val_mae: 0.3323\n",
            "Epoch 183/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3257 - val_loss: 0.1608 - val_mae: 0.3313\n",
            "Epoch 184/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1593 - mae: 0.3270 - val_loss: 0.1604 - val_mae: 0.3304\n",
            "Epoch 185/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3229 - val_loss: 0.1636 - val_mae: 0.3298\n",
            "Epoch 186/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1586 - mae: 0.3252 - val_loss: 0.1629 - val_mae: 0.3340\n",
            "Epoch 187/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1587 - mae: 0.3267 - val_loss: 0.1621 - val_mae: 0.3331\n",
            "Epoch 188/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1615 - mae: 0.3261 - val_loss: 0.1614 - val_mae: 0.3323\n",
            "Epoch 189/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1595 - mae: 0.3260 - val_loss: 0.1603 - val_mae: 0.3300\n",
            "Epoch 190/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.3245 - val_loss: 0.1603 - val_mae: 0.3294\n",
            "Epoch 191/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1599 - mae: 0.3246 - val_loss: 0.1603 - val_mae: 0.3301\n",
            "Epoch 192/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1584 - mae: 0.3252 - val_loss: 0.1604 - val_mae: 0.3291\n",
            "Epoch 193/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1595 - mae: 0.3240 - val_loss: 0.1609 - val_mae: 0.3312\n",
            "Epoch 194/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1578 - mae: 0.3242 - val_loss: 0.1640 - val_mae: 0.3345\n",
            "Epoch 195/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3233 - val_loss: 0.1599 - val_mae: 0.3290\n",
            "Epoch 196/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3243 - val_loss: 0.1634 - val_mae: 0.3337\n",
            "Epoch 197/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3238 - val_loss: 0.1623 - val_mae: 0.3284\n",
            "Epoch 198/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1615 - mae: 0.3253 - val_loss: 0.1611 - val_mae: 0.3313\n",
            "Epoch 199/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3231 - val_loss: 0.1634 - val_mae: 0.3337\n",
            "Epoch 200/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1606 - mae: 0.3245 - val_loss: 0.1610 - val_mae: 0.3312\n",
            "Epoch 201/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3242 - val_loss: 0.1600 - val_mae: 0.3281\n",
            "Epoch 202/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3188 - val_loss: 0.1683 - val_mae: 0.3385\n",
            "Epoch 203/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3251 - val_loss: 0.1601 - val_mae: 0.3280\n",
            "Epoch 204/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3219 - val_loss: 0.1670 - val_mae: 0.3370\n",
            "Epoch 205/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1602 - mae: 0.3256 - val_loss: 0.1599 - val_mae: 0.3293\n",
            "Epoch 206/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1594 - mae: 0.3243 - val_loss: 0.1604 - val_mae: 0.3279\n",
            "Epoch 207/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3212 - val_loss: 0.1631 - val_mae: 0.3331\n",
            "Epoch 208/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1592 - mae: 0.3254 - val_loss: 0.1595 - val_mae: 0.3282\n",
            "Epoch 209/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3233 - val_loss: 0.1601 - val_mae: 0.3275\n",
            "Epoch 210/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1619 - mae: 0.3240 - val_loss: 0.1597 - val_mae: 0.3288\n",
            "Epoch 211/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1584 - mae: 0.3235 - val_loss: 0.1598 - val_mae: 0.3291\n",
            "Epoch 212/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1589 - mae: 0.3239 - val_loss: 0.1605 - val_mae: 0.3304\n",
            "Epoch 213/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3229 - val_loss: 0.1662 - val_mae: 0.3363\n",
            "Epoch 214/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1594 - mae: 0.3246 - val_loss: 0.1598 - val_mae: 0.3290\n",
            "Epoch 215/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3232 - val_loss: 0.1628 - val_mae: 0.3327\n",
            "Epoch 216/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1581 - mae: 0.3250 - val_loss: 0.1594 - val_mae: 0.3280\n",
            "Epoch 217/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1594 - mae: 0.3251 - val_loss: 0.1600 - val_mae: 0.3295\n",
            "Epoch 218/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1592 - mae: 0.3239 - val_loss: 0.1596 - val_mae: 0.3286\n",
            "Epoch 219/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3208 - val_loss: 0.1614 - val_mae: 0.3312\n",
            "Epoch 220/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3230 - val_loss: 0.1593 - val_mae: 0.3269\n",
            "Epoch 221/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3222 - val_loss: 0.1591 - val_mae: 0.3274\n",
            "Epoch 222/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3240 - val_loss: 0.1597 - val_mae: 0.3265\n",
            "Epoch 223/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3233 - val_loss: 0.1600 - val_mae: 0.3264\n",
            "Epoch 224/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1581 - mae: 0.3203 - val_loss: 0.1618 - val_mae: 0.3309\n",
            "Epoch 225/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3213 - val_loss: 0.1589 - val_mae: 0.3257\n",
            "Epoch 226/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1590 - mae: 0.3215 - val_loss: 0.1589 - val_mae: 0.3270\n",
            "Epoch 227/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3224 - val_loss: 0.1588 - val_mae: 0.3260\n",
            "Epoch 228/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3230 - val_loss: 0.1591 - val_mae: 0.3256\n",
            "Epoch 229/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3228 - val_loss: 0.1586 - val_mae: 0.3260\n",
            "Epoch 230/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1579 - mae: 0.3227 - val_loss: 0.1587 - val_mae: 0.3266\n",
            "Epoch 231/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3231 - val_loss: 0.1586 - val_mae: 0.3262\n",
            "Epoch 232/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3218 - val_loss: 0.1598 - val_mae: 0.3255\n",
            "Epoch 233/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1603 - mae: 0.3202 - val_loss: 0.1633 - val_mae: 0.3320\n",
            "Epoch 234/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3233 - val_loss: 0.1589 - val_mae: 0.3269\n",
            "Epoch 235/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1603 - mae: 0.3232 - val_loss: 0.1587 - val_mae: 0.3265\n",
            "Epoch 236/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3211 - val_loss: 0.1586 - val_mae: 0.3256\n",
            "Epoch 237/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3218 - val_loss: 0.1588 - val_mae: 0.3267\n",
            "Epoch 238/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3218 - val_loss: 0.1631 - val_mae: 0.3318\n",
            "Epoch 239/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1597 - mae: 0.3227 - val_loss: 0.1671 - val_mae: 0.3361\n",
            "Epoch 240/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3240 - val_loss: 0.1590 - val_mae: 0.3272\n",
            "Epoch 241/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3218 - val_loss: 0.1585 - val_mae: 0.3260\n",
            "Epoch 242/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3194 - val_loss: 0.1602 - val_mae: 0.3288\n",
            "Epoch 243/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3229 - val_loss: 0.1586 - val_mae: 0.3250\n",
            "Epoch 244/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1578 - mae: 0.3201 - val_loss: 0.1603 - val_mae: 0.3286\n",
            "Epoch 245/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.3213 - val_loss: 0.1635 - val_mae: 0.3315\n",
            "Epoch 246/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1591 - mae: 0.3217 - val_loss: 0.1627 - val_mae: 0.3308\n",
            "Epoch 247/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3211 - val_loss: 0.1588 - val_mae: 0.3265\n",
            "Epoch 248/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3210 - val_loss: 0.1665 - val_mae: 0.3349\n",
            "Epoch 249/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.3220 - val_loss: 0.1609 - val_mae: 0.3296\n",
            "Epoch 250/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3209 - val_loss: 0.1622 - val_mae: 0.3306\n",
            "Epoch 251/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1593 - mae: 0.3217 - val_loss: 0.1618 - val_mae: 0.3303\n",
            "Epoch 252/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1584 - mae: 0.3230 - val_loss: 0.1595 - val_mae: 0.3250\n",
            "Epoch 253/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3217 - val_loss: 0.1583 - val_mae: 0.3248\n",
            "Epoch 254/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3217 - val_loss: 0.1584 - val_mae: 0.3251\n",
            "Epoch 255/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3213 - val_loss: 0.1604 - val_mae: 0.3287\n",
            "Epoch 256/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3192 - val_loss: 0.1686 - val_mae: 0.3364\n",
            "Epoch 257/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1595 - mae: 0.3222 - val_loss: 0.1588 - val_mae: 0.3266\n",
            "Epoch 258/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3219 - val_loss: 0.1624 - val_mae: 0.3308\n",
            "Epoch 259/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3224 - val_loss: 0.1598 - val_mae: 0.3283\n",
            "Epoch 260/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3229 - val_loss: 0.1585 - val_mae: 0.3242\n",
            "Epoch 261/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1574 - mae: 0.3212 - val_loss: 0.1582 - val_mae: 0.3252\n",
            "Epoch 262/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1581 - mae: 0.3211 - val_loss: 0.1581 - val_mae: 0.3249\n",
            "Epoch 263/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3199 - val_loss: 0.1591 - val_mae: 0.3242\n",
            "Epoch 264/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1596 - mae: 0.3209 - val_loss: 0.1603 - val_mae: 0.3285\n",
            "Epoch 265/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1589 - mae: 0.3224 - val_loss: 0.1582 - val_mae: 0.3241\n",
            "Epoch 266/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1592 - mae: 0.3216 - val_loss: 0.1591 - val_mae: 0.3270\n",
            "Epoch 267/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3204 - val_loss: 0.1582 - val_mae: 0.3244\n",
            "Epoch 268/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3208 - val_loss: 0.1639 - val_mae: 0.3321\n",
            "Epoch 269/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3216 - val_loss: 0.1594 - val_mae: 0.3274\n",
            "Epoch 270/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3204 - val_loss: 0.1584 - val_mae: 0.3237\n",
            "Epoch 271/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3182 - val_loss: 0.1622 - val_mae: 0.3299\n",
            "Epoch 272/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3204 - val_loss: 0.1606 - val_mae: 0.3284\n",
            "Epoch 273/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1579 - mae: 0.3200 - val_loss: 0.1592 - val_mae: 0.3269\n",
            "Epoch 274/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1574 - mae: 0.3226 - val_loss: 0.1578 - val_mae: 0.3241\n",
            "Epoch 275/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3208 - val_loss: 0.1579 - val_mae: 0.3236\n",
            "Epoch 276/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3198 - val_loss: 0.1582 - val_mae: 0.3251\n",
            "Epoch 277/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3199 - val_loss: 0.1626 - val_mae: 0.3301\n",
            "Epoch 278/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1590 - mae: 0.3211 - val_loss: 0.1633 - val_mae: 0.3309\n",
            "Epoch 279/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3201 - val_loss: 0.1585 - val_mae: 0.3258\n",
            "Epoch 280/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3203 - val_loss: 0.1587 - val_mae: 0.3259\n",
            "Epoch 281/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3185 - val_loss: 0.1623 - val_mae: 0.3297\n",
            "Epoch 282/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3200 - val_loss: 0.1607 - val_mae: 0.3280\n",
            "Epoch 283/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3200 - val_loss: 0.1608 - val_mae: 0.3238\n",
            "Epoch 284/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3176 - val_loss: 0.1654 - val_mae: 0.3328\n",
            "Epoch 285/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3200 - val_loss: 0.1581 - val_mae: 0.3248\n",
            "Epoch 286/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3201 - val_loss: 0.1593 - val_mae: 0.3268\n",
            "Epoch 287/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1598 - mae: 0.3210 - val_loss: 0.1602 - val_mae: 0.3278\n",
            "Epoch 288/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3185 - val_loss: 0.1600 - val_mae: 0.3275\n",
            "Epoch 289/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1585 - mae: 0.3195 - val_loss: 0.1599 - val_mae: 0.3274\n",
            "Epoch 290/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3200 - val_loss: 0.1578 - val_mae: 0.3242\n",
            "Epoch 291/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1598 - mae: 0.3205 - val_loss: 0.1610 - val_mae: 0.3286\n",
            "Epoch 292/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3204 - val_loss: 0.1593 - val_mae: 0.3268\n",
            "Epoch 293/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3210 - val_loss: 0.1582 - val_mae: 0.3250\n",
            "Epoch 294/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3200 - val_loss: 0.1577 - val_mae: 0.3239\n",
            "Epoch 295/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1640 - mae: 0.3212 - val_loss: 0.1628 - val_mae: 0.3299\n",
            "Epoch 296/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3200 - val_loss: 0.1582 - val_mae: 0.3248\n",
            "Epoch 297/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.3204 - val_loss: 0.1577 - val_mae: 0.3237\n",
            "Epoch 298/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3200 - val_loss: 0.1574 - val_mae: 0.3228\n",
            "Epoch 299/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3187 - val_loss: 0.1574 - val_mae: 0.3223\n",
            "Epoch 300/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3181 - val_loss: 0.1575 - val_mae: 0.3233\n",
            "Epoch 301/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3177 - val_loss: 0.1620 - val_mae: 0.3287\n",
            "Epoch 302/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3182 - val_loss: 0.1574 - val_mae: 0.3231\n",
            "Epoch 303/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3185 - val_loss: 0.1577 - val_mae: 0.3238\n",
            "Epoch 304/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3189 - val_loss: 0.1573 - val_mae: 0.3227\n",
            "Epoch 305/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3191 - val_loss: 0.1623 - val_mae: 0.3290\n",
            "Epoch 306/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3170 - val_loss: 0.1694 - val_mae: 0.3352\n",
            "Epoch 307/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1579 - mae: 0.3192 - val_loss: 0.1598 - val_mae: 0.3264\n",
            "Epoch 308/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3190 - val_loss: 0.1571 - val_mae: 0.3222\n",
            "Epoch 309/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3205 - val_loss: 0.1579 - val_mae: 0.3218\n",
            "Epoch 310/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3192 - val_loss: 0.1571 - val_mae: 0.3220\n",
            "Epoch 311/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3194 - val_loss: 0.1570 - val_mae: 0.3221\n",
            "Epoch 312/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3198 - val_loss: 0.1571 - val_mae: 0.3214\n",
            "Epoch 313/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3187 - val_loss: 0.1575 - val_mae: 0.3213\n",
            "Epoch 314/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3182 - val_loss: 0.1602 - val_mae: 0.3264\n",
            "Epoch 315/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1595 - mae: 0.3187 - val_loss: 0.1651 - val_mae: 0.3313\n",
            "Epoch 316/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3178 - val_loss: 0.1622 - val_mae: 0.3286\n",
            "Epoch 317/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3193 - val_loss: 0.1586 - val_mae: 0.3248\n",
            "Epoch 318/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3174 - val_loss: 0.1588 - val_mae: 0.3249\n",
            "Epoch 319/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3181 - val_loss: 0.1569 - val_mae: 0.3217\n",
            "Epoch 320/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3152 - val_loss: 0.1568 - val_mae: 0.3205\n",
            "Epoch 321/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3170 - val_loss: 0.1580 - val_mae: 0.3234\n",
            "Epoch 322/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1590 - mae: 0.3170 - val_loss: 0.1568 - val_mae: 0.3205\n",
            "Epoch 323/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3170 - val_loss: 0.1568 - val_mae: 0.3203\n",
            "Epoch 324/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3152 - val_loss: 0.1583 - val_mae: 0.3237\n",
            "Epoch 325/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3180 - val_loss: 0.1569 - val_mae: 0.3217\n",
            "Epoch 326/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1596 - mae: 0.3187 - val_loss: 0.1570 - val_mae: 0.3220\n",
            "Epoch 327/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1561 - mae: 0.3169 - val_loss: 0.1567 - val_mae: 0.3208\n",
            "Epoch 328/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3160 - val_loss: 0.1597 - val_mae: 0.3252\n",
            "Epoch 329/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3188 - val_loss: 0.1579 - val_mae: 0.3233\n",
            "Epoch 330/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1594 - mae: 0.3177 - val_loss: 0.1614 - val_mae: 0.3271\n",
            "Epoch 331/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3176 - val_loss: 0.1591 - val_mae: 0.3249\n",
            "Epoch 332/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3171 - val_loss: 0.1578 - val_mae: 0.3205\n",
            "Epoch 333/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3171 - val_loss: 0.1570 - val_mae: 0.3205\n",
            "Epoch 334/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3150 - val_loss: 0.1687 - val_mae: 0.3340\n",
            "Epoch 335/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3178 - val_loss: 0.1591 - val_mae: 0.3248\n",
            "Epoch 336/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3173 - val_loss: 0.1578 - val_mae: 0.3234\n",
            "Epoch 337/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3186 - val_loss: 0.1568 - val_mae: 0.3210\n",
            "Epoch 338/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3154 - val_loss: 0.1643 - val_mae: 0.3298\n",
            "Epoch 339/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1600 - mae: 0.3184 - val_loss: 0.1570 - val_mae: 0.3219\n",
            "Epoch 340/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3169 - val_loss: 0.1624 - val_mae: 0.3220\n",
            "Epoch 341/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3150 - val_loss: 0.1565 - val_mae: 0.3203\n",
            "Epoch 342/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1601 - mae: 0.3180 - val_loss: 0.1565 - val_mae: 0.3203\n",
            "Epoch 343/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3179 - val_loss: 0.1564 - val_mae: 0.3199\n",
            "Epoch 344/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3171 - val_loss: 0.1576 - val_mae: 0.3224\n",
            "Epoch 345/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3165 - val_loss: 0.1568 - val_mae: 0.3210\n",
            "Epoch 346/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3173 - val_loss: 0.1573 - val_mae: 0.3196\n",
            "Epoch 347/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1588 - mae: 0.3175 - val_loss: 0.1570 - val_mae: 0.3214\n",
            "Epoch 348/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3161 - val_loss: 0.1565 - val_mae: 0.3200\n",
            "Epoch 349/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1584 - mae: 0.3182 - val_loss: 0.1565 - val_mae: 0.3200\n",
            "Epoch 350/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3155 - val_loss: 0.1604 - val_mae: 0.3254\n",
            "Epoch 351/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3170 - val_loss: 0.1576 - val_mae: 0.3225\n",
            "Epoch 352/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3171 - val_loss: 0.1580 - val_mae: 0.3201\n",
            "Epoch 353/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3181 - val_loss: 0.1564 - val_mae: 0.3199\n",
            "Epoch 354/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3156 - val_loss: 0.1566 - val_mae: 0.3193\n",
            "Epoch 355/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3165 - val_loss: 0.1565 - val_mae: 0.3190\n",
            "Epoch 356/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3169 - val_loss: 0.1562 - val_mae: 0.3193\n",
            "Epoch 357/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3154 - val_loss: 0.1563 - val_mae: 0.3199\n",
            "Epoch 358/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3171 - val_loss: 0.1563 - val_mae: 0.3199\n",
            "Epoch 359/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3167 - val_loss: 0.1568 - val_mae: 0.3211\n",
            "Epoch 360/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1586 - mae: 0.3156 - val_loss: 0.1604 - val_mae: 0.3250\n",
            "Epoch 361/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3168 - val_loss: 0.1573 - val_mae: 0.3218\n",
            "Epoch 362/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3161 - val_loss: 0.1564 - val_mae: 0.3194\n",
            "Epoch 363/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3170 - val_loss: 0.1631 - val_mae: 0.3277\n",
            "Epoch 364/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3170 - val_loss: 0.1573 - val_mae: 0.3218\n",
            "Epoch 365/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3150 - val_loss: 0.1623 - val_mae: 0.3268\n",
            "Epoch 366/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3175 - val_loss: 0.1566 - val_mae: 0.3190\n",
            "Epoch 367/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3165 - val_loss: 0.1571 - val_mae: 0.3214\n",
            "Epoch 368/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3157 - val_loss: 0.1563 - val_mae: 0.3197\n",
            "Epoch 369/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3160 - val_loss: 0.1564 - val_mae: 0.3201\n",
            "Epoch 370/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3162 - val_loss: 0.1577 - val_mae: 0.3220\n",
            "Epoch 371/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3152 - val_loss: 0.1563 - val_mae: 0.3198\n",
            "Epoch 372/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3157 - val_loss: 0.1584 - val_mae: 0.3224\n",
            "Epoch 373/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3171 - val_loss: 0.1568 - val_mae: 0.3206\n",
            "Epoch 374/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1585 - mae: 0.3165 - val_loss: 0.1595 - val_mae: 0.3236\n",
            "Epoch 375/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1574 - mae: 0.3164 - val_loss: 0.1562 - val_mae: 0.3196\n",
            "Epoch 376/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3168 - val_loss: 0.1573 - val_mae: 0.3215\n",
            "Epoch 377/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3164 - val_loss: 0.1565 - val_mae: 0.3202\n",
            "Epoch 378/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3169 - val_loss: 0.1563 - val_mae: 0.3197\n",
            "Epoch 379/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3152 - val_loss: 0.1568 - val_mae: 0.3206\n",
            "Epoch 380/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3160 - val_loss: 0.1591 - val_mae: 0.3188\n",
            "Epoch 381/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3138 - val_loss: 0.1608 - val_mae: 0.3247\n",
            "Epoch 382/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3159 - val_loss: 0.1566 - val_mae: 0.3202\n",
            "Epoch 383/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3159 - val_loss: 0.1568 - val_mae: 0.3204\n",
            "Epoch 384/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3165 - val_loss: 0.1558 - val_mae: 0.3177\n",
            "Epoch 385/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3143 - val_loss: 0.1642 - val_mae: 0.3276\n",
            "Epoch 386/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3167 - val_loss: 0.1568 - val_mae: 0.3203\n",
            "Epoch 387/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1586 - mae: 0.3147 - val_loss: 0.1559 - val_mae: 0.3181\n",
            "Epoch 388/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3164 - val_loss: 0.1565 - val_mae: 0.3199\n",
            "Epoch 389/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3143 - val_loss: 0.1576 - val_mae: 0.3215\n",
            "Epoch 390/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1575 - mae: 0.3150 - val_loss: 0.1604 - val_mae: 0.3238\n",
            "Epoch 391/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3153 - val_loss: 0.1560 - val_mae: 0.3189\n",
            "Epoch 392/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1579 - mae: 0.3156 - val_loss: 0.1569 - val_mae: 0.3205\n",
            "Epoch 393/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3162 - val_loss: 0.1573 - val_mae: 0.3212\n",
            "Epoch 394/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3155 - val_loss: 0.1572 - val_mae: 0.3210\n",
            "Epoch 395/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1549 - mae: 0.3133 - val_loss: 0.1585 - val_mae: 0.3222\n",
            "Epoch 396/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3161 - val_loss: 0.1569 - val_mae: 0.3181\n",
            "Epoch 397/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3157 - val_loss: 0.1571 - val_mae: 0.3208\n",
            "Epoch 398/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3147 - val_loss: 0.1560 - val_mae: 0.3187\n",
            "Epoch 399/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3150 - val_loss: 0.1563 - val_mae: 0.3180\n",
            "Epoch 400/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3158 - val_loss: 0.1559 - val_mae: 0.3186\n",
            "Epoch 401/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3152 - val_loss: 0.1584 - val_mae: 0.3221\n",
            "Epoch 402/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3156 - val_loss: 0.1572 - val_mae: 0.3208\n",
            "Epoch 403/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3160 - val_loss: 0.1569 - val_mae: 0.3204\n",
            "Epoch 404/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3147 - val_loss: 0.1561 - val_mae: 0.3191\n",
            "Epoch 405/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3156 - val_loss: 0.1560 - val_mae: 0.3177\n",
            "Epoch 406/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3146 - val_loss: 0.1559 - val_mae: 0.3176\n",
            "Epoch 407/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3142 - val_loss: 0.1557 - val_mae: 0.3175\n",
            "Epoch 408/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3164 - val_loss: 0.1556 - val_mae: 0.3176\n",
            "Epoch 409/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3164 - val_loss: 0.1562 - val_mae: 0.3189\n",
            "Epoch 410/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3142 - val_loss: 0.1606 - val_mae: 0.3238\n",
            "Epoch 411/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3141 - val_loss: 0.1556 - val_mae: 0.3173\n",
            "Epoch 412/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3157 - val_loss: 0.1563 - val_mae: 0.3174\n",
            "Epoch 413/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3128 - val_loss: 0.1597 - val_mae: 0.3228\n",
            "Epoch 414/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3135 - val_loss: 0.1561 - val_mae: 0.3188\n",
            "Epoch 415/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3139 - val_loss: 0.1667 - val_mae: 0.3297\n",
            "Epoch 416/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3159 - val_loss: 0.1561 - val_mae: 0.3188\n",
            "Epoch 417/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3160 - val_loss: 0.1556 - val_mae: 0.3176\n",
            "Epoch 418/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3138 - val_loss: 0.1555 - val_mae: 0.3173\n",
            "Epoch 419/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3126 - val_loss: 0.1641 - val_mae: 0.3269\n",
            "Epoch 420/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3145 - val_loss: 0.1575 - val_mae: 0.3207\n",
            "Epoch 421/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3144 - val_loss: 0.1565 - val_mae: 0.3170\n",
            "Epoch 422/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3136 - val_loss: 0.1555 - val_mae: 0.3169\n",
            "Epoch 423/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3149 - val_loss: 0.1556 - val_mae: 0.3175\n",
            "Epoch 424/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3136 - val_loss: 0.1562 - val_mae: 0.3188\n",
            "Epoch 425/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3134 - val_loss: 0.1584 - val_mae: 0.3215\n",
            "Epoch 426/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3127 - val_loss: 0.1560 - val_mae: 0.3183\n",
            "Epoch 427/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1581 - mae: 0.3136 - val_loss: 0.1680 - val_mae: 0.3304\n",
            "Epoch 428/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3126 - val_loss: 0.1600 - val_mae: 0.3229\n",
            "Epoch 429/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3129 - val_loss: 0.1556 - val_mae: 0.3176\n",
            "Epoch 430/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1586 - mae: 0.3148 - val_loss: 0.1594 - val_mae: 0.3222\n",
            "Epoch 431/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3152 - val_loss: 0.1574 - val_mae: 0.3169\n",
            "Epoch 432/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3153 - val_loss: 0.1555 - val_mae: 0.3166\n",
            "Epoch 433/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3151 - val_loss: 0.1581 - val_mae: 0.3212\n",
            "Epoch 434/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3150 - val_loss: 0.1568 - val_mae: 0.3199\n",
            "Epoch 435/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3161 - val_loss: 0.1564 - val_mae: 0.3191\n",
            "Epoch 436/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3151 - val_loss: 0.1555 - val_mae: 0.3167\n",
            "Epoch 437/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3136 - val_loss: 0.1570 - val_mae: 0.3198\n",
            "Epoch 438/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3135 - val_loss: 0.1562 - val_mae: 0.3186\n",
            "Epoch 439/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3152 - val_loss: 0.1602 - val_mae: 0.3227\n",
            "Epoch 440/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3137 - val_loss: 0.1556 - val_mae: 0.3175\n",
            "Epoch 441/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3144 - val_loss: 0.1554 - val_mae: 0.3165\n",
            "Epoch 442/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3132 - val_loss: 0.1600 - val_mae: 0.3226\n",
            "Epoch 443/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1614 - mae: 0.3150 - val_loss: 0.1622 - val_mae: 0.3248\n",
            "Epoch 444/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3153 - val_loss: 0.1567 - val_mae: 0.3194\n",
            "Epoch 445/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3133 - val_loss: 0.1554 - val_mae: 0.3164\n",
            "Epoch 446/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3144 - val_loss: 0.1562 - val_mae: 0.3184\n",
            "Epoch 447/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3122 - val_loss: 0.1596 - val_mae: 0.3219\n",
            "Epoch 448/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3134 - val_loss: 0.1582 - val_mae: 0.3206\n",
            "Epoch 449/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3131 - val_loss: 0.1604 - val_mae: 0.3225\n",
            "Epoch 450/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3136 - val_loss: 0.1557 - val_mae: 0.3175\n",
            "Epoch 451/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1513 - mae: 0.3104 - val_loss: 0.1601 - val_mae: 0.3169\n",
            "Epoch 452/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1603 - mae: 0.3140 - val_loss: 0.1597 - val_mae: 0.3218\n",
            "Epoch 453/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3143 - val_loss: 0.1565 - val_mae: 0.3186\n",
            "Epoch 454/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3131 - val_loss: 0.1557 - val_mae: 0.3153\n",
            "Epoch 455/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3139 - val_loss: 0.1562 - val_mae: 0.3149\n",
            "Epoch 456/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3123 - val_loss: 0.1567 - val_mae: 0.3185\n",
            "Epoch 457/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3121 - val_loss: 0.1579 - val_mae: 0.3193\n",
            "Epoch 458/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3126 - val_loss: 0.1577 - val_mae: 0.3191\n",
            "Epoch 459/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3126 - val_loss: 0.1571 - val_mae: 0.3185\n",
            "Epoch 460/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3139 - val_loss: 0.1548 - val_mae: 0.3152\n",
            "Epoch 461/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3126 - val_loss: 0.1551 - val_mae: 0.3149\n",
            "Epoch 462/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3130 - val_loss: 0.1575 - val_mae: 0.3195\n",
            "Epoch 463/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3122 - val_loss: 0.1560 - val_mae: 0.3153\n",
            "Epoch 464/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3130 - val_loss: 0.1609 - val_mae: 0.3229\n",
            "Epoch 465/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3131 - val_loss: 0.1564 - val_mae: 0.3155\n",
            "Epoch 466/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3120 - val_loss: 0.1635 - val_mae: 0.3255\n",
            "Epoch 467/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3141 - val_loss: 0.1554 - val_mae: 0.3151\n",
            "Epoch 468/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3143 - val_loss: 0.1551 - val_mae: 0.3160\n",
            "Epoch 469/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1579 - mae: 0.3142 - val_loss: 0.1580 - val_mae: 0.3198\n",
            "Epoch 470/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1522 - mae: 0.3113 - val_loss: 0.1548 - val_mae: 0.3152\n",
            "Epoch 471/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3098 - val_loss: 0.1549 - val_mae: 0.3154\n",
            "Epoch 472/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3124 - val_loss: 0.1561 - val_mae: 0.3174\n",
            "Epoch 473/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3132 - val_loss: 0.1551 - val_mae: 0.3142\n",
            "Epoch 474/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3132 - val_loss: 0.1548 - val_mae: 0.3148\n",
            "Epoch 475/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3113 - val_loss: 0.1591 - val_mae: 0.3207\n",
            "Epoch 476/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3126 - val_loss: 0.1557 - val_mae: 0.3150\n",
            "Epoch 477/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3129 - val_loss: 0.1575 - val_mae: 0.3192\n",
            "Epoch 478/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3116 - val_loss: 0.1556 - val_mae: 0.3168\n",
            "Epoch 479/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3141 - val_loss: 0.1574 - val_mae: 0.3189\n",
            "Epoch 480/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3130 - val_loss: 0.1569 - val_mae: 0.3186\n",
            "Epoch 481/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3137 - val_loss: 0.1554 - val_mae: 0.3165\n",
            "Epoch 482/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3126 - val_loss: 0.1558 - val_mae: 0.3147\n",
            "Epoch 483/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3130 - val_loss: 0.1587 - val_mae: 0.3152\n",
            "Epoch 484/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3146 - val_loss: 0.1557 - val_mae: 0.3168\n",
            "Epoch 485/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3135 - val_loss: 0.1557 - val_mae: 0.3168\n",
            "Epoch 486/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3137 - val_loss: 0.1553 - val_mae: 0.3161\n",
            "Epoch 487/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3100 - val_loss: 0.1612 - val_mae: 0.3227\n",
            "Epoch 488/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3137 - val_loss: 0.1558 - val_mae: 0.3172\n",
            "Epoch 489/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1563 - mae: 0.3131 - val_loss: 0.1556 - val_mae: 0.3146\n",
            "Epoch 490/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3126 - val_loss: 0.1549 - val_mae: 0.3153\n",
            "Epoch 491/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3121 - val_loss: 0.1564 - val_mae: 0.3175\n",
            "Epoch 492/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3118 - val_loss: 0.1555 - val_mae: 0.3140\n",
            "Epoch 493/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3119 - val_loss: 0.1562 - val_mae: 0.3172\n",
            "Epoch 494/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3135 - val_loss: 0.1578 - val_mae: 0.3187\n",
            "Epoch 495/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3105 - val_loss: 0.1599 - val_mae: 0.3208\n",
            "Epoch 496/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3121 - val_loss: 0.1598 - val_mae: 0.3208\n",
            "Epoch 497/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3136 - val_loss: 0.1554 - val_mae: 0.3162\n",
            "Epoch 498/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3129 - val_loss: 0.1549 - val_mae: 0.3152\n",
            "Epoch 499/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3117 - val_loss: 0.1596 - val_mae: 0.3205\n",
            "Epoch 500/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3131 - val_loss: 0.1567 - val_mae: 0.3179\n",
            "Epoch 501/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3127 - val_loss: 0.1582 - val_mae: 0.3147\n",
            "Epoch 502/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3105 - val_loss: 0.1566 - val_mae: 0.3178\n",
            "Epoch 503/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3140 - val_loss: 0.1548 - val_mae: 0.3148\n",
            "Epoch 504/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3123 - val_loss: 0.1549 - val_mae: 0.3150\n",
            "Epoch 505/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3129 - val_loss: 0.1548 - val_mae: 0.3149\n",
            "Epoch 506/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3129 - val_loss: 0.1551 - val_mae: 0.3157\n",
            "Epoch 507/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3110 - val_loss: 0.1704 - val_mae: 0.3305\n",
            "Epoch 508/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3134 - val_loss: 0.1549 - val_mae: 0.3145\n",
            "Epoch 509/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3128 - val_loss: 0.1549 - val_mae: 0.3145\n",
            "Epoch 510/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3134 - val_loss: 0.1572 - val_mae: 0.3184\n",
            "Epoch 511/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3133 - val_loss: 0.1572 - val_mae: 0.3181\n",
            "Epoch 512/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3101 - val_loss: 0.1697 - val_mae: 0.3291\n",
            "Epoch 513/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3129 - val_loss: 0.1582 - val_mae: 0.3189\n",
            "Epoch 514/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3101 - val_loss: 0.1564 - val_mae: 0.3173\n",
            "Epoch 515/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3119 - val_loss: 0.1546 - val_mae: 0.3138\n",
            "Epoch 516/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3124 - val_loss: 0.1563 - val_mae: 0.3142\n",
            "Epoch 517/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3119 - val_loss: 0.1547 - val_mae: 0.3143\n",
            "Epoch 518/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3111 - val_loss: 0.1552 - val_mae: 0.3142\n",
            "Epoch 519/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3122 - val_loss: 0.1635 - val_mae: 0.3248\n",
            "Epoch 520/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3146 - val_loss: 0.1550 - val_mae: 0.3147\n",
            "Epoch 521/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3128 - val_loss: 0.1549 - val_mae: 0.3151\n",
            "Epoch 522/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3121 - val_loss: 0.1557 - val_mae: 0.3143\n",
            "Epoch 523/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3110 - val_loss: 0.1552 - val_mae: 0.3157\n",
            "Epoch 524/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1566 - mae: 0.3132 - val_loss: 0.1548 - val_mae: 0.3143\n",
            "Epoch 525/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3122 - val_loss: 0.1567 - val_mae: 0.3179\n",
            "Epoch 526/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3129 - val_loss: 0.1581 - val_mae: 0.3191\n",
            "Epoch 527/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3129 - val_loss: 0.1573 - val_mae: 0.3184\n",
            "Epoch 528/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3139 - val_loss: 0.1576 - val_mae: 0.3188\n",
            "Epoch 529/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3114 - val_loss: 0.1651 - val_mae: 0.3256\n",
            "Epoch 530/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3139 - val_loss: 0.1546 - val_mae: 0.3142\n",
            "Epoch 531/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3136 - val_loss: 0.1546 - val_mae: 0.3142\n",
            "Epoch 532/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3121 - val_loss: 0.1555 - val_mae: 0.3159\n",
            "Epoch 533/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3120 - val_loss: 0.1552 - val_mae: 0.3155\n",
            "Epoch 534/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1603 - mae: 0.3127 - val_loss: 0.1568 - val_mae: 0.3175\n",
            "Epoch 535/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3126 - val_loss: 0.1563 - val_mae: 0.3169\n",
            "Epoch 536/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3097 - val_loss: 0.1647 - val_mae: 0.3250\n",
            "Epoch 537/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3132 - val_loss: 0.1549 - val_mae: 0.3148\n",
            "Epoch 538/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3112 - val_loss: 0.1561 - val_mae: 0.3168\n",
            "Epoch 539/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3126 - val_loss: 0.1556 - val_mae: 0.3138\n",
            "Epoch 540/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3139 - val_loss: 0.1557 - val_mae: 0.3161\n",
            "Epoch 541/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3119 - val_loss: 0.1586 - val_mae: 0.3193\n",
            "Epoch 542/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3125 - val_loss: 0.1626 - val_mae: 0.3229\n",
            "Epoch 543/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3124 - val_loss: 0.1574 - val_mae: 0.3179\n",
            "Epoch 544/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3126 - val_loss: 0.1553 - val_mae: 0.3154\n",
            "Epoch 545/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3102 - val_loss: 0.1561 - val_mae: 0.3164\n",
            "Epoch 546/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3123 - val_loss: 0.1558 - val_mae: 0.3132\n",
            "Epoch 547/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3107 - val_loss: 0.1547 - val_mae: 0.3144\n",
            "Epoch 548/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3123 - val_loss: 0.1555 - val_mae: 0.3159\n",
            "Epoch 549/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3118 - val_loss: 0.1560 - val_mae: 0.3136\n",
            "Epoch 550/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3113 - val_loss: 0.1578 - val_mae: 0.3182\n",
            "Epoch 551/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3109 - val_loss: 0.1545 - val_mae: 0.3139\n",
            "Epoch 552/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3115 - val_loss: 0.1599 - val_mae: 0.3199\n",
            "Epoch 553/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3120 - val_loss: 0.1543 - val_mae: 0.3132\n",
            "Epoch 554/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3133 - val_loss: 0.1571 - val_mae: 0.3173\n",
            "Epoch 555/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1560 - mae: 0.3109 - val_loss: 0.1563 - val_mae: 0.3169\n",
            "Epoch 556/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3123 - val_loss: 0.1593 - val_mae: 0.3195\n",
            "Epoch 557/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3132 - val_loss: 0.1620 - val_mae: 0.3220\n",
            "Epoch 558/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3104 - val_loss: 0.1565 - val_mae: 0.3168\n",
            "Epoch 559/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3112 - val_loss: 0.1623 - val_mae: 0.3227\n",
            "Epoch 560/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3107 - val_loss: 0.1553 - val_mae: 0.3154\n",
            "Epoch 561/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3116 - val_loss: 0.1547 - val_mae: 0.3139\n",
            "Epoch 562/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3114 - val_loss: 0.1554 - val_mae: 0.3135\n",
            "Epoch 563/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3118 - val_loss: 0.1548 - val_mae: 0.3145\n",
            "Epoch 564/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3118 - val_loss: 0.1570 - val_mae: 0.3172\n",
            "Epoch 565/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3116 - val_loss: 0.1549 - val_mae: 0.3129\n",
            "Epoch 566/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3106 - val_loss: 0.1560 - val_mae: 0.3159\n",
            "Epoch 567/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3120 - val_loss: 0.1561 - val_mae: 0.3127\n",
            "Epoch 568/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3121 - val_loss: 0.1554 - val_mae: 0.3124\n",
            "Epoch 569/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3100 - val_loss: 0.1578 - val_mae: 0.3132\n",
            "Epoch 570/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3081 - val_loss: 0.1543 - val_mae: 0.3130\n",
            "Epoch 571/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3107 - val_loss: 0.1609 - val_mae: 0.3206\n",
            "Epoch 572/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3114 - val_loss: 0.1543 - val_mae: 0.3127\n",
            "Epoch 573/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3120 - val_loss: 0.1542 - val_mae: 0.3126\n",
            "Epoch 574/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3115 - val_loss: 0.1543 - val_mae: 0.3132\n",
            "Epoch 575/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3112 - val_loss: 0.1635 - val_mae: 0.3232\n",
            "Epoch 576/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3100 - val_loss: 0.1543 - val_mae: 0.3127\n",
            "Epoch 577/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3098 - val_loss: 0.1593 - val_mae: 0.3188\n",
            "Epoch 578/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3112 - val_loss: 0.1563 - val_mae: 0.3160\n",
            "Epoch 579/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3115 - val_loss: 0.1549 - val_mae: 0.3124\n",
            "Epoch 580/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1514 - mae: 0.3095 - val_loss: 0.1562 - val_mae: 0.3125\n",
            "Epoch 581/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3111 - val_loss: 0.1547 - val_mae: 0.3139\n",
            "Epoch 582/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3119 - val_loss: 0.1542 - val_mae: 0.3129\n",
            "Epoch 583/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3097 - val_loss: 0.1594 - val_mae: 0.3189\n",
            "Epoch 584/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3110 - val_loss: 0.1546 - val_mae: 0.3136\n",
            "Epoch 585/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3102 - val_loss: 0.1592 - val_mae: 0.3188\n",
            "Epoch 586/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3116 - val_loss: 0.1563 - val_mae: 0.3130\n",
            "Epoch 587/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3098 - val_loss: 0.1549 - val_mae: 0.3142\n",
            "Epoch 588/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3114 - val_loss: 0.1545 - val_mae: 0.3126\n",
            "Epoch 589/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3096 - val_loss: 0.1550 - val_mae: 0.3123\n",
            "Epoch 590/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3110 - val_loss: 0.1556 - val_mae: 0.3152\n",
            "Epoch 591/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3108 - val_loss: 0.1671 - val_mae: 0.3257\n",
            "Epoch 592/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3107 - val_loss: 0.1597 - val_mae: 0.3190\n",
            "Epoch 593/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3120 - val_loss: 0.1543 - val_mae: 0.3129\n",
            "Epoch 594/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3113 - val_loss: 0.1606 - val_mae: 0.3200\n",
            "Epoch 595/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3120 - val_loss: 0.1545 - val_mae: 0.3135\n",
            "Epoch 596/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3113 - val_loss: 0.1549 - val_mae: 0.3142\n",
            "Epoch 597/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3114 - val_loss: 0.1562 - val_mae: 0.3157\n",
            "Epoch 598/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3119 - val_loss: 0.1561 - val_mae: 0.3159\n",
            "Epoch 599/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3110 - val_loss: 0.1542 - val_mae: 0.3127\n",
            "Epoch 600/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3117 - val_loss: 0.1543 - val_mae: 0.3119\n",
            "Epoch 601/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1533 - mae: 0.3124 - val_loss: 0.1541 - val_mae: 0.3120\n",
            "Epoch 602/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3094 - val_loss: 0.1542 - val_mae: 0.3116\n",
            "Epoch 603/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3089 - val_loss: 0.1554 - val_mae: 0.3148\n",
            "Epoch 604/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3114 - val_loss: 0.1546 - val_mae: 0.3136\n",
            "Epoch 605/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3103 - val_loss: 0.1543 - val_mae: 0.3121\n",
            "Epoch 606/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3094 - val_loss: 0.1566 - val_mae: 0.3161\n",
            "Epoch 607/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3102 - val_loss: 0.1542 - val_mae: 0.3127\n",
            "Epoch 608/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1515 - mae: 0.3096 - val_loss: 0.1542 - val_mae: 0.3123\n",
            "Epoch 609/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3105 - val_loss: 0.1543 - val_mae: 0.3129\n",
            "Epoch 610/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3105 - val_loss: 0.1587 - val_mae: 0.3179\n",
            "Epoch 611/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3098 - val_loss: 0.1542 - val_mae: 0.3122\n",
            "Epoch 612/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3118 - val_loss: 0.1545 - val_mae: 0.3133\n",
            "Epoch 613/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3115 - val_loss: 0.1544 - val_mae: 0.3130\n",
            "Epoch 614/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1502 - mae: 0.3060 - val_loss: 0.1612 - val_mae: 0.3200\n",
            "Epoch 615/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3113 - val_loss: 0.1544 - val_mae: 0.3130\n",
            "Epoch 616/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3112 - val_loss: 0.1542 - val_mae: 0.3125\n",
            "Epoch 617/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3105 - val_loss: 0.1547 - val_mae: 0.3134\n",
            "Epoch 618/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3092 - val_loss: 0.1562 - val_mae: 0.3154\n",
            "Epoch 619/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3109 - val_loss: 0.1540 - val_mae: 0.3114\n",
            "Epoch 620/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3081 - val_loss: 0.1570 - val_mae: 0.3158\n",
            "Epoch 621/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3101 - val_loss: 0.1541 - val_mae: 0.3123\n",
            "Epoch 622/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1519 - mae: 0.3084 - val_loss: 0.1545 - val_mae: 0.3113\n",
            "Epoch 623/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3116 - val_loss: 0.1542 - val_mae: 0.3124\n",
            "Epoch 624/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3112 - val_loss: 0.1566 - val_mae: 0.3160\n",
            "Epoch 625/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3111 - val_loss: 0.1610 - val_mae: 0.3202\n",
            "Epoch 626/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3108 - val_loss: 0.1541 - val_mae: 0.3120\n",
            "Epoch 627/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3097 - val_loss: 0.1658 - val_mae: 0.3244\n",
            "Epoch 628/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3106 - val_loss: 0.1544 - val_mae: 0.3121\n",
            "Epoch 629/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3099 - val_loss: 0.1557 - val_mae: 0.3124\n",
            "Epoch 630/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3109 - val_loss: 0.1546 - val_mae: 0.3135\n",
            "Epoch 631/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3111 - val_loss: 0.1546 - val_mae: 0.3136\n",
            "Epoch 632/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3107 - val_loss: 0.1559 - val_mae: 0.3154\n",
            "Epoch 633/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3101 - val_loss: 0.1577 - val_mae: 0.3168\n",
            "Epoch 634/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3111 - val_loss: 0.1565 - val_mae: 0.3157\n",
            "Epoch 635/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3108 - val_loss: 0.1546 - val_mae: 0.3132\n",
            "Epoch 636/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3099 - val_loss: 0.1562 - val_mae: 0.3153\n",
            "Epoch 637/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3103 - val_loss: 0.1543 - val_mae: 0.3128\n",
            "Epoch 638/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3113 - val_loss: 0.1558 - val_mae: 0.3150\n",
            "Epoch 639/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3105 - val_loss: 0.1548 - val_mae: 0.3135\n",
            "Epoch 640/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3096 - val_loss: 0.1691 - val_mae: 0.3269\n",
            "Epoch 641/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3122 - val_loss: 0.1549 - val_mae: 0.3138\n",
            "Epoch 642/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1537 - mae: 0.3098 - val_loss: 0.1577 - val_mae: 0.3167\n",
            "Epoch 643/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3104 - val_loss: 0.1592 - val_mae: 0.3181\n",
            "Epoch 644/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3113 - val_loss: 0.1632 - val_mae: 0.3220\n",
            "Epoch 645/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3118 - val_loss: 0.1550 - val_mae: 0.3137\n",
            "Epoch 646/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3097 - val_loss: 0.1541 - val_mae: 0.3114\n",
            "Epoch 647/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3109 - val_loss: 0.1542 - val_mae: 0.3124\n",
            "Epoch 648/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3090 - val_loss: 0.1560 - val_mae: 0.3150\n",
            "Epoch 649/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3114 - val_loss: 0.1604 - val_mae: 0.3192\n",
            "Epoch 650/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3105 - val_loss: 0.1557 - val_mae: 0.3143\n",
            "Epoch 651/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3105 - val_loss: 0.1557 - val_mae: 0.3142\n",
            "Epoch 652/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3084 - val_loss: 0.1573 - val_mae: 0.3157\n",
            "Epoch 653/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1511 - mae: 0.3075 - val_loss: 0.1553 - val_mae: 0.3111\n",
            "Epoch 654/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3080 - val_loss: 0.1538 - val_mae: 0.3112\n",
            "Epoch 655/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3093 - val_loss: 0.1554 - val_mae: 0.3140\n",
            "Epoch 656/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3103 - val_loss: 0.1538 - val_mae: 0.3114\n",
            "Epoch 657/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3107 - val_loss: 0.1547 - val_mae: 0.3112\n",
            "Epoch 658/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3101 - val_loss: 0.1541 - val_mae: 0.3120\n",
            "Epoch 659/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3085 - val_loss: 0.1609 - val_mae: 0.3192\n",
            "Epoch 660/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3102 - val_loss: 0.1549 - val_mae: 0.3134\n",
            "Epoch 661/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - mae: 0.3093 - val_loss: 0.1546 - val_mae: 0.3131\n",
            "Epoch 662/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3097 - val_loss: 0.1590 - val_mae: 0.3132\n",
            "Epoch 663/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1593 - mae: 0.3109 - val_loss: 0.1546 - val_mae: 0.3132\n",
            "Epoch 664/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1524 - mae: 0.3103 - val_loss: 0.1546 - val_mae: 0.3118\n",
            "Epoch 665/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3086 - val_loss: 0.1558 - val_mae: 0.3143\n",
            "Epoch 666/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3099 - val_loss: 0.1539 - val_mae: 0.3115\n",
            "Epoch 667/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3108 - val_loss: 0.1540 - val_mae: 0.3119\n",
            "Epoch 668/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3078 - val_loss: 0.1568 - val_mae: 0.3157\n",
            "Epoch 669/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3092 - val_loss: 0.1544 - val_mae: 0.3126\n",
            "Epoch 670/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1536 - mae: 0.3095 - val_loss: 0.1546 - val_mae: 0.3129\n",
            "Epoch 671/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3079 - val_loss: 0.1538 - val_mae: 0.3113\n",
            "Epoch 672/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3098 - val_loss: 0.1549 - val_mae: 0.3134\n",
            "Epoch 673/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3087 - val_loss: 0.1539 - val_mae: 0.3114\n",
            "Epoch 674/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1521 - mae: 0.3071 - val_loss: 0.1541 - val_mae: 0.3111\n",
            "Epoch 675/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3108 - val_loss: 0.1539 - val_mae: 0.3116\n",
            "Epoch 676/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3100 - val_loss: 0.1541 - val_mae: 0.3122\n",
            "Epoch 677/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3106 - val_loss: 0.1541 - val_mae: 0.3120\n",
            "Epoch 678/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3114 - val_loss: 0.1569 - val_mae: 0.3157\n",
            "Epoch 679/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3108 - val_loss: 0.1545 - val_mae: 0.3127\n",
            "Epoch 680/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3091 - val_loss: 0.1555 - val_mae: 0.3145\n",
            "Epoch 681/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3112 - val_loss: 0.1566 - val_mae: 0.3155\n",
            "Epoch 682/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3096 - val_loss: 0.1557 - val_mae: 0.3145\n",
            "Epoch 683/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3098 - val_loss: 0.1558 - val_mae: 0.3144\n",
            "Epoch 684/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3098 - val_loss: 0.1543 - val_mae: 0.3124\n",
            "Epoch 685/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3083 - val_loss: 0.1663 - val_mae: 0.3237\n",
            "Epoch 686/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3108 - val_loss: 0.1580 - val_mae: 0.3123\n",
            "Epoch 687/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1566 - mae: 0.3108 - val_loss: 0.1558 - val_mae: 0.3145\n",
            "Epoch 688/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3106 - val_loss: 0.1571 - val_mae: 0.3156\n",
            "Epoch 689/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3098 - val_loss: 0.1540 - val_mae: 0.3113\n",
            "Epoch 690/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3079 - val_loss: 0.1560 - val_mae: 0.3143\n",
            "Epoch 691/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3093 - val_loss: 0.1540 - val_mae: 0.3108\n",
            "Epoch 692/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3098 - val_loss: 0.1554 - val_mae: 0.3136\n",
            "Epoch 693/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3098 - val_loss: 0.1539 - val_mae: 0.3116\n",
            "Epoch 694/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3097 - val_loss: 0.1540 - val_mae: 0.3117\n",
            "Epoch 695/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3105 - val_loss: 0.1559 - val_mae: 0.3146\n",
            "Epoch 696/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3086 - val_loss: 0.1605 - val_mae: 0.3191\n",
            "Epoch 697/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1521 - mae: 0.3071 - val_loss: 0.1574 - val_mae: 0.3123\n",
            "Epoch 698/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3078 - val_loss: 0.1578 - val_mae: 0.3164\n",
            "Epoch 699/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3110 - val_loss: 0.1542 - val_mae: 0.3121\n",
            "Epoch 700/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1585 - mae: 0.3091 - val_loss: 0.1549 - val_mae: 0.3133\n",
            "Epoch 701/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1567 - mae: 0.3112 - val_loss: 0.1548 - val_mae: 0.3135\n",
            "Epoch 702/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3095 - val_loss: 0.1599 - val_mae: 0.3187\n",
            "Epoch 703/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3083 - val_loss: 0.1542 - val_mae: 0.3122\n",
            "Epoch 704/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3092 - val_loss: 0.1540 - val_mae: 0.3112\n",
            "Epoch 705/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3097 - val_loss: 0.1540 - val_mae: 0.3113\n",
            "Epoch 706/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1546 - mae: 0.3089 - val_loss: 0.1583 - val_mae: 0.3170\n",
            "Epoch 707/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3092 - val_loss: 0.1545 - val_mae: 0.3130\n",
            "Epoch 708/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3098 - val_loss: 0.1558 - val_mae: 0.3145\n",
            "Epoch 709/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3095 - val_loss: 0.1581 - val_mae: 0.3166\n",
            "Epoch 710/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3106 - val_loss: 0.1542 - val_mae: 0.3121\n",
            "Epoch 711/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3102 - val_loss: 0.1542 - val_mae: 0.3122\n",
            "Epoch 712/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3086 - val_loss: 0.1548 - val_mae: 0.3116\n",
            "Epoch 713/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3102 - val_loss: 0.1541 - val_mae: 0.3109\n",
            "Epoch 714/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3077 - val_loss: 0.1609 - val_mae: 0.3194\n",
            "Epoch 715/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3102 - val_loss: 0.1542 - val_mae: 0.3124\n",
            "Epoch 716/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3094 - val_loss: 0.1557 - val_mae: 0.3144\n",
            "Epoch 717/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1580 - mae: 0.3113 - val_loss: 0.1542 - val_mae: 0.3122\n",
            "Epoch 718/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3081 - val_loss: 0.1615 - val_mae: 0.3202\n",
            "Epoch 719/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3075 - val_loss: 0.1547 - val_mae: 0.3112\n",
            "Epoch 720/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1585 - mae: 0.3096 - val_loss: 0.1561 - val_mae: 0.3146\n",
            "Epoch 721/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3088 - val_loss: 0.1541 - val_mae: 0.3120\n",
            "Epoch 722/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3075 - val_loss: 0.1616 - val_mae: 0.3197\n",
            "Epoch 723/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3076 - val_loss: 0.1562 - val_mae: 0.3145\n",
            "Epoch 724/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3099 - val_loss: 0.1539 - val_mae: 0.3113\n",
            "Epoch 725/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3095 - val_loss: 0.1540 - val_mae: 0.3117\n",
            "Epoch 726/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1574 - mae: 0.3104 - val_loss: 0.1551 - val_mae: 0.3132\n",
            "Epoch 727/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3096 - val_loss: 0.1550 - val_mae: 0.3133\n",
            "Epoch 728/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3103 - val_loss: 0.1547 - val_mae: 0.3128\n",
            "Epoch 729/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3084 - val_loss: 0.1539 - val_mae: 0.3113\n",
            "Epoch 730/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3101 - val_loss: 0.1562 - val_mae: 0.3143\n",
            "Epoch 731/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3109 - val_loss: 0.1537 - val_mae: 0.3100\n",
            "Epoch 732/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3081 - val_loss: 0.1565 - val_mae: 0.3145\n",
            "Epoch 733/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3076 - val_loss: 0.1540 - val_mae: 0.3116\n",
            "Epoch 734/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3084 - val_loss: 0.1538 - val_mae: 0.3109\n",
            "Epoch 735/1000\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1537 - mae: 0.3108 - val_loss: 0.1538 - val_mae: 0.3112\n",
            "Epoch 736/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3083 - val_loss: 0.1602 - val_mae: 0.3182\n",
            "Epoch 737/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1523 - mae: 0.3060 - val_loss: 0.1540 - val_mae: 0.3108\n",
            "Epoch 738/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3089 - val_loss: 0.1549 - val_mae: 0.3129\n",
            "Epoch 739/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3084 - val_loss: 0.1539 - val_mae: 0.3104\n",
            "Epoch 740/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3094 - val_loss: 0.1538 - val_mae: 0.3111\n",
            "Epoch 741/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3086 - val_loss: 0.1579 - val_mae: 0.3159\n",
            "Epoch 742/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1524 - mae: 0.3105 - val_loss: 0.1538 - val_mae: 0.3110\n",
            "Epoch 743/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3093 - val_loss: 0.1540 - val_mae: 0.3115\n",
            "Epoch 744/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3091 - val_loss: 0.1550 - val_mae: 0.3130\n",
            "Epoch 745/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3082 - val_loss: 0.1559 - val_mae: 0.3137\n",
            "Epoch 746/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3111 - val_loss: 0.1540 - val_mae: 0.3114\n",
            "Epoch 747/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3072 - val_loss: 0.1569 - val_mae: 0.3106\n",
            "Epoch 748/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3091 - val_loss: 0.1551 - val_mae: 0.3100\n",
            "Epoch 749/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3081 - val_loss: 0.1575 - val_mae: 0.3148\n",
            "Epoch 750/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3080 - val_loss: 0.1535 - val_mae: 0.3097\n",
            "Epoch 751/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3090 - val_loss: 0.1539 - val_mae: 0.3102\n",
            "Epoch 752/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1576 - mae: 0.3104 - val_loss: 0.1555 - val_mae: 0.3134\n",
            "Epoch 753/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3097 - val_loss: 0.1563 - val_mae: 0.3143\n",
            "Epoch 754/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3093 - val_loss: 0.1548 - val_mae: 0.3126\n",
            "Epoch 755/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3094 - val_loss: 0.1538 - val_mae: 0.3108\n",
            "Epoch 756/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3085 - val_loss: 0.1543 - val_mae: 0.3117\n",
            "Epoch 757/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3087 - val_loss: 0.1539 - val_mae: 0.3112\n",
            "Epoch 758/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3069 - val_loss: 0.1569 - val_mae: 0.3145\n",
            "Epoch 759/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3094 - val_loss: 0.1544 - val_mae: 0.3118\n",
            "Epoch 760/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3059 - val_loss: 0.1659 - val_mae: 0.3227\n",
            "Epoch 761/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3107 - val_loss: 0.1538 - val_mae: 0.3111\n",
            "Epoch 762/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3092 - val_loss: 0.1540 - val_mae: 0.3113\n",
            "Epoch 763/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1571 - mae: 0.3103 - val_loss: 0.1549 - val_mae: 0.3132\n",
            "Epoch 764/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3097 - val_loss: 0.1546 - val_mae: 0.3124\n",
            "Epoch 765/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3098 - val_loss: 0.1554 - val_mae: 0.3134\n",
            "Epoch 766/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1522 - mae: 0.3078 - val_loss: 0.1539 - val_mae: 0.3111\n",
            "Epoch 767/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1572 - mae: 0.3102 - val_loss: 0.1562 - val_mae: 0.3143\n",
            "Epoch 768/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3103 - val_loss: 0.1545 - val_mae: 0.3124\n",
            "Epoch 769/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3075 - val_loss: 0.1576 - val_mae: 0.3153\n",
            "Epoch 770/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1573 - mae: 0.3097 - val_loss: 0.1542 - val_mae: 0.3118\n",
            "Epoch 771/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3090 - val_loss: 0.1539 - val_mae: 0.3106\n",
            "Epoch 772/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3089 - val_loss: 0.1552 - val_mae: 0.3133\n",
            "Epoch 773/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3095 - val_loss: 0.1539 - val_mae: 0.3113\n",
            "Epoch 774/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3092 - val_loss: 0.1538 - val_mae: 0.3109\n",
            "Epoch 775/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3100 - val_loss: 0.1541 - val_mae: 0.3117\n",
            "Epoch 776/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3082 - val_loss: 0.1568 - val_mae: 0.3148\n",
            "Epoch 777/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3089 - val_loss: 0.1541 - val_mae: 0.3116\n",
            "Epoch 778/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3095 - val_loss: 0.1538 - val_mae: 0.3109\n",
            "Epoch 779/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3084 - val_loss: 0.1571 - val_mae: 0.3147\n",
            "Epoch 780/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3097 - val_loss: 0.1538 - val_mae: 0.3108\n",
            "Epoch 781/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3087 - val_loss: 0.1538 - val_mae: 0.3108\n",
            "Epoch 782/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3094 - val_loss: 0.1582 - val_mae: 0.3155\n",
            "Epoch 783/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1574 - mae: 0.3103 - val_loss: 0.1552 - val_mae: 0.3129\n",
            "Epoch 784/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1526 - mae: 0.3090 - val_loss: 0.1547 - val_mae: 0.3103\n",
            "Epoch 785/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3071 - val_loss: 0.1539 - val_mae: 0.3103\n",
            "Epoch 786/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1519 - mae: 0.3073 - val_loss: 0.1698 - val_mae: 0.3257\n",
            "Epoch 787/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3079 - val_loss: 0.1540 - val_mae: 0.3104\n",
            "Epoch 788/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1518 - mae: 0.3071 - val_loss: 0.1544 - val_mae: 0.3102\n",
            "Epoch 789/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3084 - val_loss: 0.1571 - val_mae: 0.3147\n",
            "Epoch 790/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3077 - val_loss: 0.1609 - val_mae: 0.3185\n",
            "Epoch 791/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3065 - val_loss: 0.1539 - val_mae: 0.3111\n",
            "Epoch 792/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3089 - val_loss: 0.1595 - val_mae: 0.3167\n",
            "Epoch 793/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3077 - val_loss: 0.1639 - val_mae: 0.3212\n",
            "Epoch 794/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3090 - val_loss: 0.1564 - val_mae: 0.3145\n",
            "Epoch 795/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3100 - val_loss: 0.1553 - val_mae: 0.3133\n",
            "Epoch 796/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3078 - val_loss: 0.1563 - val_mae: 0.3144\n",
            "Epoch 797/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3093 - val_loss: 0.1609 - val_mae: 0.3187\n",
            "Epoch 798/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3088 - val_loss: 0.1558 - val_mae: 0.3139\n",
            "Epoch 799/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3072 - val_loss: 0.1644 - val_mae: 0.3222\n",
            "Epoch 800/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3106 - val_loss: 0.1580 - val_mae: 0.3164\n",
            "Epoch 801/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3110 - val_loss: 0.1544 - val_mae: 0.3124\n",
            "Epoch 802/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3098 - val_loss: 0.1552 - val_mae: 0.3136\n",
            "Epoch 803/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3094 - val_loss: 0.1642 - val_mae: 0.3217\n",
            "Epoch 804/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3108 - val_loss: 0.1546 - val_mae: 0.3126\n",
            "Epoch 805/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3069 - val_loss: 0.1600 - val_mae: 0.3180\n",
            "Epoch 806/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1524 - mae: 0.3073 - val_loss: 0.1565 - val_mae: 0.3150\n",
            "Epoch 807/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3100 - val_loss: 0.1564 - val_mae: 0.3147\n",
            "Epoch 808/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3097 - val_loss: 0.1546 - val_mae: 0.3125\n",
            "Epoch 809/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3095 - val_loss: 0.1564 - val_mae: 0.3146\n",
            "Epoch 810/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3074 - val_loss: 0.1564 - val_mae: 0.3148\n",
            "Epoch 811/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3085 - val_loss: 0.1648 - val_mae: 0.3222\n",
            "Epoch 812/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3108 - val_loss: 0.1545 - val_mae: 0.3124\n",
            "Epoch 813/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3093 - val_loss: 0.1539 - val_mae: 0.3111\n",
            "Epoch 814/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3080 - val_loss: 0.1540 - val_mae: 0.3099\n",
            "Epoch 815/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3089 - val_loss: 0.1576 - val_mae: 0.3149\n",
            "Epoch 816/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3076 - val_loss: 0.1590 - val_mae: 0.3164\n",
            "Epoch 817/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1523 - mae: 0.3085 - val_loss: 0.1539 - val_mae: 0.3110\n",
            "Epoch 818/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3095 - val_loss: 0.1549 - val_mae: 0.3125\n",
            "Epoch 819/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3095 - val_loss: 0.1538 - val_mae: 0.3099\n",
            "Epoch 820/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3080 - val_loss: 0.1581 - val_mae: 0.3152\n",
            "Epoch 821/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3068 - val_loss: 0.1538 - val_mae: 0.3103\n",
            "Epoch 822/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3095 - val_loss: 0.1542 - val_mae: 0.3115\n",
            "Epoch 823/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3097 - val_loss: 0.1541 - val_mae: 0.3115\n",
            "Epoch 824/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3083 - val_loss: 0.1540 - val_mae: 0.3113\n",
            "Epoch 825/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3085 - val_loss: 0.1537 - val_mae: 0.3105\n",
            "Epoch 826/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3080 - val_loss: 0.1545 - val_mae: 0.3119\n",
            "Epoch 827/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3084 - val_loss: 0.1539 - val_mae: 0.3110\n",
            "Epoch 828/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3083 - val_loss: 0.1536 - val_mae: 0.3102\n",
            "Epoch 829/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3078 - val_loss: 0.1599 - val_mae: 0.3170\n",
            "Epoch 830/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3085 - val_loss: 0.1535 - val_mae: 0.3098\n",
            "Epoch 831/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3087 - val_loss: 0.1535 - val_mae: 0.3096\n",
            "Epoch 832/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3099 - val_loss: 0.1536 - val_mae: 0.3094\n",
            "Epoch 833/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3077 - val_loss: 0.1539 - val_mae: 0.3095\n",
            "Epoch 834/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3067 - val_loss: 0.1539 - val_mae: 0.3109\n",
            "Epoch 835/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3066 - val_loss: 0.1541 - val_mae: 0.3096\n",
            "Epoch 836/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1514 - mae: 0.3075 - val_loss: 0.1546 - val_mae: 0.3119\n",
            "Epoch 837/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3085 - val_loss: 0.1591 - val_mae: 0.3162\n",
            "Epoch 838/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3071 - val_loss: 0.1575 - val_mae: 0.3150\n",
            "Epoch 839/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3082 - val_loss: 0.1623 - val_mae: 0.3199\n",
            "Epoch 840/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3107 - val_loss: 0.1544 - val_mae: 0.3105\n",
            "Epoch 841/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3087 - val_loss: 0.1540 - val_mae: 0.3096\n",
            "Epoch 842/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3098 - val_loss: 0.1535 - val_mae: 0.3094\n",
            "Epoch 843/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3092 - val_loss: 0.1578 - val_mae: 0.3146\n",
            "Epoch 844/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1562 - mae: 0.3090 - val_loss: 0.1564 - val_mae: 0.3136\n",
            "Epoch 845/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3084 - val_loss: 0.1547 - val_mae: 0.3117\n",
            "Epoch 846/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3094 - val_loss: 0.1535 - val_mae: 0.3097\n",
            "Epoch 847/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1553 - mae: 0.3092 - val_loss: 0.1565 - val_mae: 0.3136\n",
            "Epoch 848/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1518 - mae: 0.3090 - val_loss: 0.1568 - val_mae: 0.3138\n",
            "Epoch 849/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3087 - val_loss: 0.1553 - val_mae: 0.3125\n",
            "Epoch 850/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3091 - val_loss: 0.1547 - val_mae: 0.3095\n",
            "Epoch 851/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1557 - mae: 0.3073 - val_loss: 0.1541 - val_mae: 0.3110\n",
            "Epoch 852/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3073 - val_loss: 0.1535 - val_mae: 0.3100\n",
            "Epoch 853/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3087 - val_loss: 0.1536 - val_mae: 0.3102\n",
            "Epoch 854/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3061 - val_loss: 0.1587 - val_mae: 0.3155\n",
            "Epoch 855/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3086 - val_loss: 0.1540 - val_mae: 0.3107\n",
            "Epoch 856/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3077 - val_loss: 0.1582 - val_mae: 0.3150\n",
            "Epoch 857/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3070 - val_loss: 0.1565 - val_mae: 0.3137\n",
            "Epoch 858/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3086 - val_loss: 0.1543 - val_mae: 0.3094\n",
            "Epoch 859/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3068 - val_loss: 0.1586 - val_mae: 0.3154\n",
            "Epoch 860/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3082 - val_loss: 0.1537 - val_mae: 0.3093\n",
            "Epoch 861/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3084 - val_loss: 0.1536 - val_mae: 0.3101\n",
            "Epoch 862/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3085 - val_loss: 0.1539 - val_mae: 0.3107\n",
            "Epoch 863/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3079 - val_loss: 0.1605 - val_mae: 0.3178\n",
            "Epoch 864/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3095 - val_loss: 0.1568 - val_mae: 0.3143\n",
            "Epoch 865/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3074 - val_loss: 0.1624 - val_mae: 0.3197\n",
            "Epoch 866/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3104 - val_loss: 0.1551 - val_mae: 0.3128\n",
            "Epoch 867/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3090 - val_loss: 0.1538 - val_mae: 0.3101\n",
            "Epoch 868/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3082 - val_loss: 0.1553 - val_mae: 0.3130\n",
            "Epoch 869/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1514 - mae: 0.3062 - val_loss: 0.1535 - val_mae: 0.3096\n",
            "Epoch 870/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3080 - val_loss: 0.1535 - val_mae: 0.3095\n",
            "Epoch 871/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3067 - val_loss: 0.1592 - val_mae: 0.3162\n",
            "Epoch 872/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3095 - val_loss: 0.1570 - val_mae: 0.3142\n",
            "Epoch 873/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3072 - val_loss: 0.1664 - val_mae: 0.3227\n",
            "Epoch 874/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3090 - val_loss: 0.1538 - val_mae: 0.3098\n",
            "Epoch 875/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3084 - val_loss: 0.1538 - val_mae: 0.3105\n",
            "Epoch 876/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3094 - val_loss: 0.1535 - val_mae: 0.3095\n",
            "Epoch 877/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1564 - mae: 0.3077 - val_loss: 0.1538 - val_mae: 0.3106\n",
            "Epoch 878/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3096 - val_loss: 0.1549 - val_mae: 0.3126\n",
            "Epoch 879/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3078 - val_loss: 0.1540 - val_mae: 0.3111\n",
            "Epoch 880/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3092 - val_loss: 0.1554 - val_mae: 0.3132\n",
            "Epoch 881/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3067 - val_loss: 0.1621 - val_mae: 0.3194\n",
            "Epoch 882/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3071 - val_loss: 0.1584 - val_mae: 0.3156\n",
            "Epoch 883/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3094 - val_loss: 0.1537 - val_mae: 0.3104\n",
            "Epoch 884/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3069 - val_loss: 0.1586 - val_mae: 0.3158\n",
            "Epoch 885/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1570 - mae: 0.3095 - val_loss: 0.1564 - val_mae: 0.3140\n",
            "Epoch 886/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3091 - val_loss: 0.1550 - val_mae: 0.3126\n",
            "Epoch 887/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3092 - val_loss: 0.1553 - val_mae: 0.3128\n",
            "Epoch 888/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3085 - val_loss: 0.1545 - val_mae: 0.3103\n",
            "Epoch 889/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3091 - val_loss: 0.1573 - val_mae: 0.3145\n",
            "Epoch 890/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3082 - val_loss: 0.1674 - val_mae: 0.3233\n",
            "Epoch 891/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3085 - val_loss: 0.1579 - val_mae: 0.3147\n",
            "Epoch 892/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3078 - val_loss: 0.1536 - val_mae: 0.3095\n",
            "Epoch 893/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3065 - val_loss: 0.1609 - val_mae: 0.3179\n",
            "Epoch 894/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3097 - val_loss: 0.1537 - val_mae: 0.3095\n",
            "Epoch 895/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3072 - val_loss: 0.1569 - val_mae: 0.3141\n",
            "Epoch 896/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1524 - mae: 0.3080 - val_loss: 0.1549 - val_mae: 0.3100\n",
            "Epoch 897/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3091 - val_loss: 0.1548 - val_mae: 0.3120\n",
            "Epoch 898/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1554 - mae: 0.3095 - val_loss: 0.1536 - val_mae: 0.3100\n",
            "Epoch 899/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3067 - val_loss: 0.1534 - val_mae: 0.3093\n",
            "Epoch 900/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3077 - val_loss: 0.1545 - val_mae: 0.3088\n",
            "Epoch 901/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1523 - mae: 0.3054 - val_loss: 0.1541 - val_mae: 0.3106\n",
            "Epoch 902/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3068 - val_loss: 0.1533 - val_mae: 0.3092\n",
            "Epoch 903/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3072 - val_loss: 0.1535 - val_mae: 0.3096\n",
            "Epoch 904/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1583 - mae: 0.3090 - val_loss: 0.1557 - val_mae: 0.3126\n",
            "Epoch 905/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1509 - mae: 0.3078 - val_loss: 0.1543 - val_mae: 0.3091\n",
            "Epoch 906/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3065 - val_loss: 0.1567 - val_mae: 0.3134\n",
            "Epoch 907/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1501 - mae: 0.3057 - val_loss: 0.1585 - val_mae: 0.3107\n",
            "Epoch 908/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3091 - val_loss: 0.1555 - val_mae: 0.3124\n",
            "Epoch 909/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3078 - val_loss: 0.1560 - val_mae: 0.3132\n",
            "Epoch 910/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1560 - mae: 0.3093 - val_loss: 0.1533 - val_mae: 0.3088\n",
            "Epoch 911/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3073 - val_loss: 0.1538 - val_mae: 0.3101\n",
            "Epoch 912/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1556 - mae: 0.3080 - val_loss: 0.1548 - val_mae: 0.3114\n",
            "Epoch 913/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3078 - val_loss: 0.1550 - val_mae: 0.3092\n",
            "Epoch 914/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3073 - val_loss: 0.1537 - val_mae: 0.3089\n",
            "Epoch 915/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3081 - val_loss: 0.1536 - val_mae: 0.3099\n",
            "Epoch 916/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1582 - mae: 0.3072 - val_loss: 0.1548 - val_mae: 0.3115\n",
            "Epoch 917/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3069 - val_loss: 0.1557 - val_mae: 0.3124\n",
            "Epoch 918/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3082 - val_loss: 0.1557 - val_mae: 0.3125\n",
            "Epoch 919/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3079 - val_loss: 0.1570 - val_mae: 0.3136\n",
            "Epoch 920/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3080 - val_loss: 0.1540 - val_mae: 0.3103\n",
            "Epoch 921/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3062 - val_loss: 0.1534 - val_mae: 0.3089\n",
            "Epoch 922/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3085 - val_loss: 0.1548 - val_mae: 0.3119\n",
            "Epoch 923/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3061 - val_loss: 0.1601 - val_mae: 0.3167\n",
            "Epoch 924/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3079 - val_loss: 0.1538 - val_mae: 0.3090\n",
            "Epoch 925/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1569 - mae: 0.3091 - val_loss: 0.1536 - val_mae: 0.3098\n",
            "Epoch 926/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3079 - val_loss: 0.1561 - val_mae: 0.3127\n",
            "Epoch 927/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3090 - val_loss: 0.1536 - val_mae: 0.3087\n",
            "Epoch 928/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1521 - mae: 0.3061 - val_loss: 0.1579 - val_mae: 0.3143\n",
            "Epoch 929/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3096 - val_loss: 0.1540 - val_mae: 0.3105\n",
            "Epoch 930/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3082 - val_loss: 0.1539 - val_mae: 0.3101\n",
            "Epoch 931/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3070 - val_loss: 0.1534 - val_mae: 0.3094\n",
            "Epoch 932/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3077 - val_loss: 0.1579 - val_mae: 0.3145\n",
            "Epoch 933/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3086 - val_loss: 0.1535 - val_mae: 0.3094\n",
            "Epoch 934/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3084 - val_loss: 0.1546 - val_mae: 0.3110\n",
            "Epoch 935/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3065 - val_loss: 0.1533 - val_mae: 0.3089\n",
            "Epoch 936/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1512 - mae: 0.3067 - val_loss: 0.1541 - val_mae: 0.3085\n",
            "Epoch 937/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1561 - mae: 0.3077 - val_loss: 0.1535 - val_mae: 0.3085\n",
            "Epoch 938/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3054 - val_loss: 0.1578 - val_mae: 0.3141\n",
            "Epoch 939/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3074 - val_loss: 0.1533 - val_mae: 0.3090\n",
            "Epoch 940/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1547 - mae: 0.3075 - val_loss: 0.1588 - val_mae: 0.3153\n",
            "Epoch 941/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3079 - val_loss: 0.1555 - val_mae: 0.3089\n",
            "Epoch 942/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3083 - val_loss: 0.1543 - val_mae: 0.3104\n",
            "Epoch 943/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3080 - val_loss: 0.1531 - val_mae: 0.3085\n",
            "Epoch 944/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1559 - mae: 0.3079 - val_loss: 0.1536 - val_mae: 0.3084\n",
            "Epoch 945/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1535 - mae: 0.3082 - val_loss: 0.1533 - val_mae: 0.3090\n",
            "Epoch 946/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3072 - val_loss: 0.1540 - val_mae: 0.3104\n",
            "Epoch 947/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1551 - mae: 0.3082 - val_loss: 0.1547 - val_mae: 0.3091\n",
            "Epoch 948/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1568 - mae: 0.3078 - val_loss: 0.1633 - val_mae: 0.3198\n",
            "Epoch 949/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3085 - val_loss: 0.1538 - val_mae: 0.3106\n",
            "Epoch 950/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1544 - mae: 0.3070 - val_loss: 0.1578 - val_mae: 0.3148\n",
            "Epoch 951/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3072 - val_loss: 0.1624 - val_mae: 0.3123\n",
            "Epoch 952/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3080 - val_loss: 0.1534 - val_mae: 0.3091\n",
            "Epoch 953/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1526 - mae: 0.3048 - val_loss: 0.1581 - val_mae: 0.3144\n",
            "Epoch 954/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3086 - val_loss: 0.1540 - val_mae: 0.3103\n",
            "Epoch 955/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1563 - mae: 0.3088 - val_loss: 0.1534 - val_mae: 0.3089\n",
            "Epoch 956/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1519 - mae: 0.3065 - val_loss: 0.1563 - val_mae: 0.3130\n",
            "Epoch 957/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1564 - mae: 0.3074 - val_loss: 0.1574 - val_mae: 0.3140\n",
            "Epoch 958/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1527 - mae: 0.3078 - val_loss: 0.1548 - val_mae: 0.3114\n",
            "Epoch 959/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3075 - val_loss: 0.1536 - val_mae: 0.3092\n",
            "Epoch 960/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3074 - val_loss: 0.1600 - val_mae: 0.3169\n",
            "Epoch 961/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1534 - mae: 0.3074 - val_loss: 0.1535 - val_mae: 0.3097\n",
            "Epoch 962/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3074 - val_loss: 0.1539 - val_mae: 0.3103\n",
            "Epoch 963/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1550 - mae: 0.3078 - val_loss: 0.1537 - val_mae: 0.3101\n",
            "Epoch 964/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1540 - mae: 0.3073 - val_loss: 0.1535 - val_mae: 0.3089\n",
            "Epoch 965/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1537 - mae: 0.3072 - val_loss: 0.1536 - val_mae: 0.3097\n",
            "Epoch 966/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3071 - val_loss: 0.1614 - val_mae: 0.3179\n",
            "Epoch 967/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1517 - mae: 0.3073 - val_loss: 0.1537 - val_mae: 0.3090\n",
            "Epoch 968/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3065 - val_loss: 0.1540 - val_mae: 0.3091\n",
            "Epoch 969/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1538 - mae: 0.3072 - val_loss: 0.1574 - val_mae: 0.3143\n",
            "Epoch 970/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1549 - mae: 0.3092 - val_loss: 0.1542 - val_mae: 0.3093\n",
            "Epoch 971/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3059 - val_loss: 0.1580 - val_mae: 0.3146\n",
            "Epoch 972/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3069 - val_loss: 0.1549 - val_mae: 0.3117\n",
            "Epoch 973/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3080 - val_loss: 0.1539 - val_mae: 0.3090\n",
            "Epoch 974/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3080 - val_loss: 0.1536 - val_mae: 0.3097\n",
            "Epoch 975/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1529 - mae: 0.3079 - val_loss: 0.1540 - val_mae: 0.3103\n",
            "Epoch 976/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1528 - mae: 0.3065 - val_loss: 0.1539 - val_mae: 0.3100\n",
            "Epoch 977/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3068 - val_loss: 0.1548 - val_mae: 0.3113\n",
            "Epoch 978/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1536 - mae: 0.3077 - val_loss: 0.1538 - val_mae: 0.3101\n",
            "Epoch 979/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1533 - mae: 0.3061 - val_loss: 0.1535 - val_mae: 0.3095\n",
            "Epoch 980/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1541 - mae: 0.3067 - val_loss: 0.1539 - val_mae: 0.3104\n",
            "Epoch 981/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1532 - mae: 0.3074 - val_loss: 0.1533 - val_mae: 0.3090\n",
            "Epoch 982/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1552 - mae: 0.3066 - val_loss: 0.1540 - val_mae: 0.3104\n",
            "Epoch 983/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1518 - mae: 0.3076 - val_loss: 0.1587 - val_mae: 0.3152\n",
            "Epoch 984/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3069 - val_loss: 0.1567 - val_mae: 0.3136\n",
            "Epoch 985/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1542 - mae: 0.3070 - val_loss: 0.1556 - val_mae: 0.3122\n",
            "Epoch 986/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1530 - mae: 0.3076 - val_loss: 0.1549 - val_mae: 0.3118\n",
            "Epoch 987/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1577 - mae: 0.3098 - val_loss: 0.1566 - val_mae: 0.3137\n",
            "Epoch 988/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1539 - mae: 0.3079 - val_loss: 0.1619 - val_mae: 0.3186\n",
            "Epoch 989/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3093 - val_loss: 0.1577 - val_mae: 0.3144\n",
            "Epoch 990/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1543 - mae: 0.3083 - val_loss: 0.1542 - val_mae: 0.3109\n",
            "Epoch 991/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1516 - mae: 0.3045 - val_loss: 0.1655 - val_mae: 0.3211\n",
            "Epoch 992/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1546 - mae: 0.3068 - val_loss: 0.1607 - val_mae: 0.3173\n",
            "Epoch 993/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1545 - mae: 0.3089 - val_loss: 0.1536 - val_mae: 0.3099\n",
            "Epoch 994/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1517 - mae: 0.3077 - val_loss: 0.1538 - val_mae: 0.3103\n",
            "Epoch 995/1000\n",
            "38/38 [==============================] - 0s 3ms/step - loss: 0.1554 - mae: 0.3075 - val_loss: 0.1548 - val_mae: 0.3116\n",
            "Epoch 996/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1531 - mae: 0.3071 - val_loss: 0.1546 - val_mae: 0.3115\n",
            "Epoch 997/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1558 - mae: 0.3092 - val_loss: 0.1538 - val_mae: 0.3105\n",
            "Epoch 998/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1525 - mae: 0.3062 - val_loss: 0.1536 - val_mae: 0.3096\n",
            "Epoch 999/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1555 - mae: 0.3086 - val_loss: 0.1539 - val_mae: 0.3088\n",
            "Epoch 1000/1000\n",
            "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3076 - val_loss: 0.1550 - val_mae: 0.3116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWnooB595pff",
        "colab_type": "text"
      },
      "source": [
        "Time to explore the arguments of the `fit()` function's arguments:\n",
        "\n",
        "`x_train, y_train`\n",
        "\n",
        "The first two arguments to `fit()` are the x and y values of our training data. Remember that parts of our data are kept aside for validation and testing, so only the training set is used to train the network.\n",
        "\n",
        "`epochs`\n",
        "\n",
        "The next argument specifies how many times our entire training set will be run through the network during training. The more epochs, the more training will occur. **You might think that the more training happens, the better the network will be. However, some networks will start to overfit their training data after a certain number of epochs, so we might want to limit the amount of training we do. In addition, even if there's no overfitting, a network will stop improving after a certain amount of training.** Since training costs time and computational resources, it's best not to train if the network isn't getting better.\n",
        "\n",
        "We're starting out with 1,000 epochs of training. When training is complete, we can dig into our metrics to discover whether this is the correct number.\n",
        "\n",
        "`batch_size`\n",
        "\n",
        "The `batch_size` argument specifies how many pieces of training data to feed into the network before measuring its accuracy and updating its weights and biases. If we wanted, we could specify a `batch_size` of `1`, meaning we'd run inference on a single datapoint, measuring the loss of the network's prediction, update the weights and biases to make the prediction more accurate next time, and then continue this cycle for the rest of the data. \n",
        "\n",
        "**Because we have 600 datapoints, each epoch would result in 600 updates to the network. This is a lot of computation, so our training would take ages.** An alternative might be to select and run inference on multiple datapoints, measure the loss in aggregate, and then update the network accordingly.\n",
        "\n",
        "If we set `batch_size` to `600`, each batch would include all of our training data. We'd now have to make only one update to the network every epoch - much quicker. The problem is that this results in less accurate models. Research has shown that models trained with large batch sizes have less ability to generalize to new data - they are more likely to overfit.\n",
        "\n",
        "The compromise is to use a small batch size - start with something like 16 or 32 and experiment from there to see what works best. \n",
        "\n",
        "`validation_data`\n",
        "\n",
        "This is where we specify our validation dataset. Data from this dataset will be run through the network throughout the training process, and the network's predictions will be compared with the expected values. We'll see the results of validation in the logs and as part of the `history_1` object.\n",
        "\n",
        "# Training Metrics\n",
        "Time to check the metrics to see how well our network has learned. To begin, let's look at the logs written during training. This will show how the network has improved during training from its random initial state.\n",
        "\n",
        "The log for our very first epoch is:\n",
        "`Epoch 1/1000\n",
        "38/38 [==============================] - 0s 6ms/step - loss: 0.4094 - mae: 0.5590 - val_loss: 0.4553 - val_mae: 0.5831`\n",
        "\n",
        "The log for our very last epoch is:\n",
        "`Epoch 1000/1000\n",
        "38/38 [==============================] - 0s 2ms/step - loss: 0.1548 - mae: 0.3076 - val_loss: 0.1550 - val_mae: 0.3116`\n",
        "\n",
        "The `loss`, `mae`, and `val_mae` tell us various things:\n",
        "\n",
        "`loss`\n",
        "\n",
        "This is the output of our loss function. We're using mean squared error, which is expressed as a positive number. Generally, the smaller the loss value, the better, so this is a good thing to watch as we evaluate our network.\n",
        "\n",
        "Comparing the first and last epochs, the network has clearly improved during training, going from a loss of ~0.4 to ~0.15\n",
        "\n",
        "`mae` \n",
        "\n",
        "This is the mean absolute error of our training data. It shows the average difference between the network's predictions and the expected y values from the training data. \n",
        "\n",
        "We can expect our initial error to be pretty dismal, given that it's based on an untrained network. This is confirmed - our MAE is ~0.56 (range of -1 to 1). This is large. But even after training, our mse is ~0.3 which is still quite awful.\n",
        "\n",
        "`val_loss`\n",
        "\n",
        "This is the output of our loss function on our validation data. In our final epoch, the training loss (\\~0.15) is slightly lower than the validation loss (\\~0.17). This is a hint that our network might be overfitting, because it is performing worse on data it has not seen before. \n",
        "\n",
        "`val_mse` \n",
        "\n",
        "This is the mse for our validation data. With a value of 0.32, it's worse than the mse on our training set, which is another sign of overfitting. \n",
        "\n",
        "# Graphing the history\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGSUymRH7Zb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}